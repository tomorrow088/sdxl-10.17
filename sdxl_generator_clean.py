"""
SDXLè¿·å½©ç”Ÿæˆå™¨æ¨¡å— - ç²¾ç®€ç‰ˆ
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from PIL import Image
from typing import List, Optional, Tuple, Dict
import os
import random
from torchvision import transforms
from diffusers import (
    StableDiffusionXLPipeline,        # æ–‡æœ¬åˆ°å›¾åƒ
    StableDiffusionXLImg2ImgPipeline,        # å›¾åƒåˆ°å›¾åƒ
    StableDiffusionXLInpaintPipeline,            # å›¾åƒä¿®å¤
    DDIMScheduler        # ç¯å¢ƒæ„ŸçŸ¥è°ƒåº¦å™¨
)
from transformers import CLIPModel, CLIPProcessor
from peft import LoraConfig, get_peft_model

from config import model_config, training_config
from sdxl_advanced_adapter import create_sdxl_adapter



class SDXLCamouflageGenerator(nn.Module):
    """SDXLè¿·å½©ç”Ÿæˆå™¨ - ç²¾ç®€ç‰ˆ"""
    
    def __init__(self, 
                 model_path: str = None,
                 device: str = "cuda",
                 use_lora: bool = True,
                 use_smart_adapter: bool = True):
        """
        åˆå§‹åŒ–SDXLè¿·å½©ç”Ÿæˆå™¨
        
        Args:
            model_path: SDXLæ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            use_lora: æ˜¯å¦ä½¿ç”¨LoRAè®­ç»ƒ
            use_smart_adapter: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½é€‚é…å™¨
        """
        super().__init__()
        
        self.device = device
        self.model_path = model_path or model_config.sdxl_model_path
        self.use_lora = use_lora
        self.use_smart_adapter = use_smart_adapter
        
        # ç¼“å­˜ä¼˜åŒ–ï¼šé¿å…é‡å¤åˆå§‹åŒ–
        self._trainable_params_cached = False
        self._cached_params = []
        
        # åŠ è½½ç®¡é“
        self._load_pipelines()
        
        # åˆå§‹åŒ–CLIPï¼ˆç”¨äºå‚è€ƒå›¾åƒé€‰æ‹©ï¼‰
        self._init_clip()
        
        # è®¾ç½®LoRA
        if use_lora:
            self._setup_lora()
        
        # å‚è€ƒå›¾æ¡ˆç¼“å­˜
        self.reference_db = []
        self._load_reference_patterns()

    def set_training_mode(self, is_training: bool = True) -> None:
        """ä¸è®­ç»ƒå™¨æ¥å£å…¼å®¹çš„æ¨¡å¼åˆ‡æ¢
        - è®­ç»ƒæ—¶ï¼šä»…å°†å¯è®­ç»ƒæ¨¡å—è®¾ä¸ºtrainï¼ˆadapterã€unet_loraï¼‰
        - å…¶ä½™å¤§æ¨¡å‹ï¼ˆVAEã€æ–‡æœ¬ç¼–ç å™¨ï¼‰ä¿æŒevalï¼Œé¿å…ä¸å¿…è¦çš„dropout/BNæ‰°åŠ¨
        """
        # æœ¬æ¨¡å—æœ¬èº«çš„nn.Moduleå¼€å…³ï¼ˆä¸å½±å“diffuserså­æ¨¡å—ï¼‰
        super().train(is_training)
        
        # 1) é€‚é…å™¨
        if hasattr(self, 'adapter') and self.adapter is not None:
            self.adapter.train(is_training)
        
        # 2) LoRA-UNetï¼ˆè®­ç»ƒç”¨ï¼‰
        if hasattr(self, 'unet_lora') and self.unet_lora is not None:
            self.unet_lora.train(is_training)
        
        # 3) å¤§æ¨¡å‹ç»„ä»¶ä¿æŒevalï¼Œé™¤éæ˜ç¡®éœ€è¦è®­ç»ƒ
        pipe = getattr(self, 'txt2img_pipe', None)
        if pipe is not None:
            # æ–‡æœ¬ç¼–ç å™¨
            if hasattr(pipe, 'text_encoder') and pipe.text_encoder is not None:
                pipe.text_encoder.eval()
            if hasattr(pipe, 'text_encoder_2') and pipe.text_encoder_2 is not None:
                pipe.text_encoder_2.eval()
            # VAE/UNet
            if hasattr(pipe, 'vae') and pipe.vae is not None:
                pipe.vae.eval()
            if hasattr(pipe, 'unet') and pipe.unet is not None:
                pipe.unet.eval()
        
        # å…¶ä»–å¤ç”¨ç®¡é“åŒæ ·ä¿æŒeval
        for aux_pipe_name in ['img2img_pipe', 'inpaint_pipe']:
            aux_pipe = getattr(self, aux_pipe_name, None)
            if aux_pipe is not None:
                if hasattr(aux_pipe, 'vae') and aux_pipe.vae is not None:
                    aux_pipe.vae.eval()
                if hasattr(aux_pipe, 'unet') and aux_pipe.unet is not None:
                    aux_pipe.unet.eval()
                if hasattr(aux_pipe, 'text_encoder') and aux_pipe.text_encoder is not None:
                    aux_pipe.text_encoder.eval()
                if hasattr(aux_pipe, 'text_encoder_2') and aux_pipe.text_encoder_2 is not None:
                    aux_pipe.text_encoder_2.eval()

    def _load_pipelines(self):
        """åŠ è½½SDXLç®¡é“ï¼ˆç²¾ç®€å†…å­˜å ç”¨ç‰ˆæœ¬ï¼‰"""
        print(f"ğŸ”„ åŠ è½½SDXLæ¨¡å‹: {self.model_path}")
        
        # ä»…åŠ è½½ä¸€ä¸ªä¸»ç®¡çº¿ï¼Œé¿å…é‡å¤å ç”¨æ˜¾å­˜
        # å°½é‡ä»¥åŠç²¾åº¦åŠ è½½ä»¥èŠ‚çœæ˜¾å­˜
        load_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        self.txt2img_pipe = StableDiffusionXLPipeline.from_pretrained(
            self.model_path,
            torch_dtype=load_dtype,
            use_safetensors=True,
            variant="fp16" if load_dtype == torch.float16 else None
        )
        # å¼ é‡å¸ƒå±€ä¼˜åŒ–
        try:
            self.txt2img_pipe.unet.to(memory_format=torch.channels_last)
            self.txt2img_pipe.vae.to(memory_format=torch.channels_last)
            torch.backends.cudnn.benchmark = True
        except Exception:
            pass
        
        # å¼€å¯çœæ˜¾å­˜é€‰é¡¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            if hasattr(self.txt2img_pipe, 'enable_attention_slicing'):
                self.txt2img_pipe.enable_attention_slicing("max")
                print("âœ… å·²å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡")
            # xFormerså†…å­˜ä¼˜åŒ–ï¼ˆè‹¥å¯ç”¨ï¼‰
            try:
                if hasattr(self.txt2img_pipe, 'enable_xformers_memory_efficient_attention'):
                    self.txt2img_pipe.enable_xformers_memory_efficient_attention()
                    print("âœ… å·²å¯ç”¨xFormerså†…å­˜ä¼˜åŒ–")
            except Exception:
                pass
            if hasattr(self.txt2img_pipe, 'vae') and hasattr(self.txt2img_pipe.vae, 'enable_tiling'):
                self.txt2img_pipe.vae.enable_tiling()
                print("âœ… å·²å¯ç”¨VAEå¹³é“º")
        except Exception:
            pass
        
        # å°†ä¸»ç®¡çº¿ç§»åŠ¨åˆ°è®¾å¤‡
        self.txt2img_pipe = self.txt2img_pipe.to(self.device)

        # å…¼å®¹æ—§è®­ç»ƒå™¨ï¼šæŒ‚æ¥å¸¸ç”¨å­æ¨¡å—å¼•ç”¨
        self.unet = self.txt2img_pipe.unet
        self.vae = self.txt2img_pipe.vae
        self.text_encoder = getattr(self.txt2img_pipe, 'text_encoder', None)
        self.text_encoder_2 = getattr(self.txt2img_pipe, 'text_encoder_2', None)

        # æ¢¯åº¦æ£€æŸ¥ç‚¹å·²ç¦ç”¨ä»¥æå‡è®­ç»ƒé€Ÿåº¦ï¼ˆç‰ºç‰²éƒ¨åˆ†æ˜¾å­˜ï¼‰
        # try:
        #     if hasattr(self.unet, 'enable_gradient_checkpointing'):
        #         self.unet.enable_gradient_checkpointing()
        #         print("âœ… å·²å¯ç”¨UNetæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œé™ä½æ˜¾å­˜å ç”¨")
        # except Exception:
        #     pass

        # ä¸å†åˆ›å»ºé¢å¤–çš„ç®¡çº¿ï¼ŒèŠ‚çœæ˜¾å­˜ï¼ˆè®­ç»ƒç”¨ä¸åˆ°ï¼‰
        self.img2img_pipe = None
        self.inpaint_pipe = None
        
        # å™ªå£°è°ƒåº¦å™¨
        self.noise_scheduler = DDIMScheduler.from_config(self.txt2img_pipe.scheduler.config)
        
        # ä¸ºè®­ç»ƒç¨³å®šæ€§ï¼Œå°†VAEåˆ‡æ¢åˆ°FP32ç²¾åº¦ï¼ˆé¿å…Convåä¼ NaNï¼‰
        try:
            self.vae = self.vae.to(dtype=torch.float32)
            print("âœ… VAEå·²åˆ‡æ¢åˆ°float32ä»¥å¢å¼ºæ•°å€¼ç¨³å®šæ€§")
        except Exception:
            pass
        
        print("âœ… SDXLç®¡é“åŠ è½½å®Œæˆï¼ˆç²¾ç®€æ¨¡å¼ï¼‰")
    
    def _init_clip(self):
        """åˆå§‹åŒ–CLIPæ¨¡å‹"""
        # å°è¯•æœ¬åœ°CLIPæ¨¡å‹è·¯å¾„
        local_clip_paths = [
            "models/laion-CLIP-ViT-H-14-laion2B-s32B-b79K",  # é¦–é€‰ï¼šå®Œæ•´çš„HFæ ¼å¼
            "models/ViT-L-14.pt",  # å¤‡é€‰ï¼šOpenCLIPæ ¼å¼
            "openai/clip-vit-base-patch32"  # æœ€åå¤‡é€‰ï¼šåœ¨çº¿ä¸‹è½½
        ]
        
        for clip_path in local_clip_paths:
            try:
                if clip_path.endswith('.pt'):
                    # OpenCLIPæ ¼å¼ - éœ€è¦ç‰¹æ®Šå¤„ç†
                    print(f"ğŸ”„ å°è¯•åŠ è½½OpenCLIPæ¨¡å‹: {clip_path}")
                    import open_clip
                    model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained=clip_path)
                    self.clip_model = model.to(self.device)
                    self.clip_processor = preprocess
                    print(f"âœ… OpenCLIPæ¨¡å‹åŠ è½½æˆåŠŸ: {clip_path}")
                    self._clip_type = 'openclip'
                    return
                else:
                    # Hugging Faceæ ¼å¼
                    print(f"ğŸ”„ å°è¯•åŠ è½½HF CLIPæ¨¡å‹: {clip_path}")
                    if os.path.exists(clip_path):
                        self.clip_model = CLIPModel.from_pretrained(clip_path, local_files_only=True).to(self.device)
                        self.clip_processor = CLIPProcessor.from_pretrained(clip_path, local_files_only=True)
                    else:
                        self.clip_model = CLIPModel.from_pretrained(clip_path).to(self.device)
                        self.clip_processor = CLIPProcessor.from_pretrained(clip_path)
                    
                    print(f"âœ… HF CLIPæ¨¡å‹åŠ è½½æˆåŠŸ: {clip_path}")
                    self._clip_type = 'huggingface'
                    return
                    
            except Exception as e:
                print(f"âš ï¸ CLIPæ¨¡å‹åŠ è½½å¤±è´¥ {clip_path}: {e}")
                continue
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        print("âŒ æ‰€æœ‰CLIPæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¦ç”¨CLIPåŠŸèƒ½")
        self.clip_model = None
        self.clip_processor = None
        self._clip_type = None
    
    def _setup_lora(self):
        """è®¾ç½®LoRA"""
        lora_rank = getattr(model_config, 'lora_rank', 8)
        lora_config = LoraConfig(
            r=lora_rank,
            lora_alpha=32,
            target_modules=["to_k", "to_q", "to_v", "to_out.0"],
            lora_dropout=0.1,
        )

        self.unet_lora = get_peft_model(self.txt2img_pipe.unet, lora_config)
        print("âœ… LoRAè®¾ç½®å®Œæˆ")
    
    def _load_reference_patterns(self):
        """åŠ è½½å‚è€ƒå›¾æ¡ˆ"""
        ref_dir = getattr(training_config, 'reference_patterns_dir', None)
        if ref_dir and os.path.isdir(ref_dir):
            for file in os.listdir(ref_dir)[:20]:  # é™åˆ¶æ•°é‡
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    try:
                        img_path = os.path.join(ref_dir, file)
                        img = Image.open(img_path).convert('RGB')
                        self.reference_db.append({
                            'image': img,
                            'path': img_path,
                            'tensor': None  # å»¶è¿ŸåŠ è½½
                        })
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½å‚è€ƒå›¾æ¡ˆå¤±è´¥ {file}: {e}")
            
            print(f"âœ… åŠ è½½äº† {len(self.reference_db)} ä¸ªå‚è€ƒå›¾æ¡ˆ")

    def get_trainable_parameters(self):
        """è·å–å¯è®­ç»ƒå‚æ•°ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        # å¦‚æœå·²ç¼“å­˜ï¼Œç›´æ¥è¿”å›
        if self._trainable_params_cached and self._cached_params:
            return self._cached_params

        trainable_params = []

        # åˆ›å»ºé€‚é…å™¨ï¼ˆä»…é¦–æ¬¡ï¼‰
        if not hasattr(self, 'adapter'):
            # 1. ä» config.py è¯»å–ä¸¤ä¸ªåŠŸèƒ½å¼€å…³
            use_reference = getattr(training_config, 'adapter_use_reference', False)

            # 2. å°†ä¸¤ä¸ªå¼€å…³åŒæ—¶ä¼ é€’ç»™å·¥å‚å‡½æ•°
            self.adapter = create_sdxl_adapter(
                smart=self.use_smart_adapter,
                use_reference=use_reference
            )

            self.adapter = self.adapter.to(self.device)
            self.add_module('adapter', self.adapter)

            # ç‰¹å¾æŠ•å½±å±‚ - ä½¿ç”¨æ›´å¤æ‚çš„æŠ•å½±æ¥ä¿ç•™é¢œè‰²ä¿¡æ¯
            self.feature_projector = nn.ModuleList([
                nn.Sequential(
                    nn.Conv2d(320, 64, 3, 1, 1),
                    nn.ReLU(),
                    nn.Conv2d(64, 3, 3, 1, 1)
                ),   # 64x64
                nn.Sequential(
                    nn.Conv2d(640, 128, 3, 1, 1),
                    nn.ReLU(),
                    nn.Conv2d(128, 3, 3, 1, 1)
                ),   # 32x32
                nn.Sequential(
                    nn.Conv2d(1280, 256, 3, 1, 1),
                    nn.ReLU(),
                    nn.Conv2d(256, 3, 3, 1, 1)
                ),  # 16x16
                nn.Sequential(
                    nn.Conv2d(1280, 256, 3, 1, 1),
                    nn.ReLU(),
                    nn.Conv2d(256, 3, 3, 1, 1)
                ),  # 8x8
            ]).to(self.device)
            self.add_module('feature_projector', self.feature_projector)

            # é€‚é…å™¨â†’UNet æ®‹å·®æ³¨å…¥æŠ•å½±ï¼ˆT2I-Adapterå¼æ³¨å…¥ï¼‰
            # å°†å„å°ºåº¦é€‚é…å™¨ç‰¹å¾æŠ•å½±åˆ°ä¸UNet down blocks å¯¹é½çš„é€šé“æ•°
            self.adapter_residual_proj_down = nn.ModuleList([
                nn.Conv2d(320, 320, 1, 1, 0),
                nn.Conv2d(640, 640, 1, 1, 0),
                nn.Conv2d(1280, 1280, 1, 1, 0),
                nn.Conv2d(1280, 1280, 1, 1, 0)
            ]).to(self.device)
            self.adapter_residual_proj_mid = nn.Conv2d(1280, 1280, 1, 1, 0).to(self.device)
            self.add_module('adapter_residual_proj_down', self.adapter_residual_proj_down)
            self.add_module('adapter_residual_proj_mid', self.adapter_residual_proj_mid)

        # æ”¶é›†å¯è®­ç»ƒå‚æ•°
        if self.use_lora and hasattr(self, 'unet_lora'):
            trainable_params.extend([p for p in self.unet_lora.parameters() if p.requires_grad])
        
        trainable_params.extend(list(self.adapter.parameters()))
        trainable_params.extend(list(self.feature_projector.parameters()))
        # æ³¨å…¥æŠ•å½±å±‚ä¸€å¹¶è®­ç»ƒ
        if hasattr(self, 'adapter_residual_proj_down'):
            trainable_params.extend(list(self.adapter_residual_proj_down.parameters()))
        if hasattr(self, 'adapter_residual_proj_mid'):
            trainable_params.extend(list(self.adapter_residual_proj_mid.parameters()))
        
        # ç¼“å­˜å‚æ•°åˆ—è¡¨
        self._cached_params = trainable_params
        self._trainable_params_cached = True
        
        print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in trainable_params):,}")
        return trainable_params

    def apply_adapter_fast_training(self, batch_images: torch.Tensor, batch_masks: torch.Tensor, 
                                   prompts: Optional[List[str]] = None) -> torch.Tensor:
        """
        å¿«é€Ÿé€‚é…å™¨è®­ç»ƒæ¨¡å¼ - ä¸è¿›è¡Œæ‰©æ•£é‡‡æ ·ï¼Œç›´æ¥ç”Ÿæˆè¿·å½©
        å¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦ï¼Œé™ä½æ˜¾å­˜ä½¿ç”¨
        """
        if not hasattr(self, 'adapter'):
            self.get_trainable_parameters()

        # è§„èŒƒåŒ–è¾“å…¥
        batch_images = batch_images.to(self.device)
        batch_masks = batch_masks.to(self.device).float()
        
        if batch_masks.max() > 1.0:
            batch_masks = batch_masks / 255.0
        
        # è°ƒæ•´å°ºå¯¸
        h, w = batch_images.shape[-2], batch_images.shape[-1]
        if batch_masks.shape[-2:] != (h, w):
            batch_masks = F.interpolate(batch_masks, size=(h, w), mode='nearest')

        # è½¬æ¢åˆ°[0,1]èŒƒå›´
        images_01 = (batch_images + 1.0) / 2.0
        
        # å‡†å¤‡æ–‡æœ¬æ¡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
        text_embeds = None
        time_embeds = None
        
        if prompts is not None:
            with torch.no_grad():
                cond1, cond2, pooled2 = self._encode_prompts(prompts)
                cond1_pooled = cond1.mean(dim=1)
                text_embeds = torch.cat([pooled2, cond1_pooled], dim=-1)  # [B, 2048]
                
                B = batch_images.shape[0]
                time_embeds = torch.tensor(
                    [h, w, 0, 0, h, w], 
                    device=self.device, dtype=torch.float32
                ).unsqueeze(0).repeat(B, 1)

        adapter_input = torch.cat([images_01, batch_masks], dim=1)
        ref_tensor = None

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å‚è€ƒé€‚é…å™¨
        if isinstance(self.adapter, getattr(__import__('sdxl_advanced_adapter'), 'SDXLAdapterWithReference', object)):
            # (ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸ºæ•´ä¸ªæ‰¹æ¬¡é€‰æ‹©ä¸€ä¸ªæœ€ä½³å‚è€ƒå›¾)
            # 1. å°†è¾“å…¥è½¬æ¢ä¸ºPILå›¾åƒç”¨äºCLIPåˆ†æ
            first_image_pil = self._tensor_to_pil(batch_images[0])
            first_mask_pil = self._tensor_to_pil(batch_masks[0, 0], is_mask=True)

            # 2. è°ƒç”¨CLIPé€‰æ‹©æœ€ä½³å‚è€ƒå›¾
            best_ref = self._select_best_reference_by_clip(first_image_pil, first_mask_pil)

            # 3. é¢„å¤„ç†å‚è€ƒå›¾å¹¶æ„å»ºæ‰¹æ¬¡
            if best_ref and 'image' in best_ref:
                ref_pil = best_ref['image'].resize((w, h))
                # è½¬æ¢ä¸º [-1, 1] èŒƒå›´çš„å¼ é‡
                ref_tensor_single = ((transforms.ToTensor()(ref_pil) * 2 - 1)).to(self.device)
                # æ‰©å±•åˆ°æ•´ä¸ªæ‰¹æ¬¡
                ref_tensor = ref_tensor_single.unsqueeze(0).repeat(batch_images.shape[0], 1, 1, 1)

        # é€‚é…å™¨è¾“å…¥
        adapter_input = torch.cat([images_01, batch_masks], dim=1)
        
        # ç›´æ¥é€šè¿‡é€‚é…å™¨ç”Ÿæˆç‰¹å¾
        if hasattr(self.adapter, 'forward'):
            features = self.adapter(adapter_input, ref=ref_tensor, text_embeds=text_embeds, time_embeds=time_embeds)
        else:
            features = self.adapter(adapter_input)

        # é€šè¿‡ç‰¹å¾æŠ•å½±å±‚ç”Ÿæˆæœ€ç»ˆè¿·å½©
        # é€‰æ‹©æœ€é«˜åˆ†è¾¨ç‡çš„ç‰¹å¾ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªï¼‰
        main_feature = features[0]  # [B, 320, H/4, W/4]
        
        # æŠ•å½±åˆ°RGB
        camo_raw = self.feature_projector[0](main_feature)  # [B, 3, H/4, W/4]
        
        # ä¸Šé‡‡æ ·åˆ°åŸå§‹åˆ†è¾¨ç‡
        camo = F.interpolate(camo_raw, size=(h, w), mode='bilinear', align_corners=False)
        
        # åº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œç¡®ä¿è¾“å‡ºåœ¨[-1,1]èŒƒå›´
        camo = torch.tanh(camo)
        
        # æŒ‰æ©ç èåˆï¼šæ©ç åŒºåŸŸä½¿ç”¨ç”Ÿæˆçš„è¿·å½©ï¼Œéæ©ç åŒºåŸŸä¿æŒåŸå›¾
        blend_alpha = 0.9  # å¼ºæ··åˆï¼Œç¡®ä¿è¿·å½©æ•ˆæœæ˜æ˜¾
        result = batch_images * (1 - batch_masks * blend_alpha) + camo * (batch_masks * blend_alpha)
        
        # ç¡®ä¿è¾“å‡ºèŒƒå›´æ­£ç¡®
        result = torch.clamp(result, -1, 1)
        
        return result

    def apply_adapter(self, batch_images: torch.Tensor, batch_masks: torch.Tensor, 
                     prompts: Optional[List[str]] = None) -> torch.Tensor:
        """
        åº”ç”¨é€‚é…å™¨ç”Ÿæˆè¿·å½©
        
        Args:
            batch_images: [B, 3, H, W] è¾“å…¥å›¾åƒï¼ŒèŒƒå›´[-1,1]
            batch_masks: [B, 1, H, W] æ©ç 
            prompts: æç¤ºè¯åˆ—è¡¨
        
        Returns:
            ç”Ÿæˆçš„å›¾åƒ [B, 3, H, W]
        """
        if not hasattr(self, 'adapter'):
            self.get_trainable_parameters()

        # è§„èŒƒåŒ–è¾“å…¥
        batch_images = batch_images.to(self.device)
        batch_masks = batch_masks.to(self.device).float()
        
        if batch_masks.max() > 1.0:
            batch_masks = batch_masks / 255.0
        
        # è°ƒæ•´å°ºå¯¸
        h, w = batch_images.shape[-2], batch_images.shape[-1]
        if batch_masks.shape[-2:] != (h, w):
            batch_masks = F.interpolate(batch_masks, size=(h, w), mode='nearest')

        # è½¬æ¢åˆ°[0,1]èŒƒå›´
        images_01 = (batch_images + 1.0) / 2.0
        
        # å‡†å¤‡æ–‡æœ¬æ¡ä»¶
        text_embeds = None
        time_embeds = None
        
        if prompts is not None:
            with torch.no_grad():
                cond1, cond2, pooled2 = self._encode_prompts(prompts)
                cond1_pooled = cond1.mean(dim=1)
                text_embeds = torch.cat([pooled2, cond1_pooled], dim=-1)  # [B, 2048]
                
                B = batch_images.shape[0]
                time_embeds = torch.tensor(
                    [h, w, 0, 0, h, w], 
                    device=self.device, dtype=torch.float32
                ).unsqueeze(0).repeat(B, 1)

        # é€‚é…å™¨è¾“å…¥
        adapter_input = torch.cat([images_01, batch_masks], dim=1)
        
        # -------- æ–¹æ¡ˆBï¼šé€šè¿‡ added_cond_kwargs å‘UNetæ³¨å…¥é€‚é…å™¨æ®‹å·®ï¼Œå¹¶è¿›è¡Œç®€åŒ–å¤šæ­¥é‡‡æ · --------
        # 1) å…ˆå–é€‚é…å™¨å¤šå°ºåº¦ç‰¹å¾
        if hasattr(self.adapter, 'forward'):
            features = self.adapter(adapter_input, text_embeds=text_embeds, time_embeds=time_embeds)
        else:
            features = self.adapter(adapter_input)

        # 2) æ„é€  down_block_additional_residuals ä¸ mid_block_additional_residual
        unet = self.unet
        unet_dtype = next(unet.parameters()).dtype
        down_residuals = []
        for i in range(min(len(features), len(self.adapter_residual_proj_down))):
            feat = features[i]
            proj = self.adapter_residual_proj_down[i](feat).to(unet_dtype)
            down_residuals.append(proj)
        mid_residual = self.adapter_residual_proj_mid(features[-1]).to(unet_dtype)

        # 3) æ–‡æœ¬æ¡ä»¶
        if prompts is not None:
            with torch.no_grad():
                cond1, cond2, pooled2 = self._encode_prompts(prompts)
                encoder_hidden_states = torch.cat([cond2, cond1], dim=-1).to(unet_dtype)
                pooled2 = pooled2.to(unet_dtype)
        else:
            # æ— æç¤ºè¯åˆ™ä½¿ç”¨é›¶å‘é‡ï¼ˆä¿æŒå½¢çŠ¶æ­£ç¡®ï¼‰
            B = batch_images.shape[0]
            device = self.device
            hs = 2048
            encoder_hidden_states = torch.zeros((B, 77, hs), device=device, dtype=unet_dtype)
            pooled2 = torch.zeros((B, 1280), device=device, dtype=unet_dtype)

        target_h, target_w = model_config.image_size[1], model_config.image_size[0]
        time_ids = torch.tensor([target_h, target_w, 0, 0, target_h, target_w], 
                               device=self.device, dtype=unet_dtype).repeat(batch_images.shape[0], 1)

        added = {
            'text_embeds': pooled2,
            'time_ids': time_ids,
            'down_block_additional_residuals': [r for r in down_residuals],
            'mid_block_additional_residual': mid_residual
        }

        # 4) ä»¥å™ªå£°ä¸ºèµ·ç‚¹åšç®€åŒ–DDIMé‡‡æ ·ï¼ˆå°‘æ­¥æ•°ï¼‰ï¼Œå¾—åˆ°çº¹ç†å†æŒ‰æ©ç æ··åˆå›å»
        vae = self.txt2img_pipe.vae
        scheduler = self.noise_scheduler
        # é‡‡æ ·ä»çº¯å™ªå£°å¼€å§‹ï¼ˆå¦‚éœ€img2imgæ•ˆæœï¼Œå¯æ”¹ä¸ºä¸å›¾åƒlatentæŒ‰strengthæ··åˆï¼‰
        with torch.no_grad():
            B, _, H, W = batch_images.shape
            latent_h, latent_w = H // 8, W // 8
            latents = torch.randn((B, vae.config.latent_channels if hasattr(vae, 'config') else 4, latent_h, latent_w),
                                  device=self.device, dtype=unet_dtype)
            latents = latents * scheduler.init_noise_sigma

        # æ­¥æ•°ï¼šè®­ç»ƒ/éªŒè¯åˆ†å¼€é…ç½®
        num_steps = int(training_config.train_num_steps if self.training else training_config.eval_num_steps)
        scheduler.set_timesteps(num_steps, device=self.device)
        latents = latents.to(unet_dtype)
        # classifier-free guidanceï¼šè®­ç»ƒæ—¶ç®€åŒ–ä¸ºå•å‰å‘ï¼ŒéªŒè¯æ—¶ä½¿ç”¨å®Œæ•´CFG
        guidance_scale = getattr(model_config, 'guidance_scale', 7.5)
        
        if self.training:
            # è®­ç»ƒæ—¶ï¼šä»…æœ‰æ¡ä»¶å‰å‘ï¼ˆå¤§å¹…æé€Ÿï¼‰
            for t in scheduler.timesteps:
                noise_pred = unet(latents, t, encoder_hidden_states=encoder_hidden_states, added_cond_kwargs=added).sample
                latents = scheduler.step(noise_pred, t, latents).prev_sample
        else:
            # éªŒè¯æ—¶ï¼šå®Œæ•´CFGä»¥ä¿è¯è´¨é‡
            uncond = torch.zeros_like(encoder_hidden_states)
            with torch.no_grad():
                for t in scheduler.timesteps:
                    noise_pred_uncond = unet(latents, t, encoder_hidden_states=uncond, added_cond_kwargs=added).sample
                    noise_pred_text = unet(latents, t, encoder_hidden_states=encoder_hidden_states, added_cond_kwargs=added).sample
                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
                    latents = scheduler.step(noise_pred, t, latents).prev_sample

        # åœ¨è§£ç å‰é‡Šæ”¾éƒ¨åˆ†ä¸­é—´å¼ é‡ï¼ˆä¿ç•™å¿…è¦çš„æ˜¾å­˜æ¸…ç†ï¼‰
        try:
            if 'noise_pred_uncond' in locals():
                del noise_pred_uncond
            if 'noise_pred_text' in locals():
                del noise_pred_text
            if 'uncond' in locals():
                del uncond
        except Exception:
            pass

        # è§£ç å‰æ¸…ç†æ˜¾å­˜ç¢ç‰‡
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è§£ç åˆ°å›¾åƒï¼šè®­ç»ƒä¿ç•™æ¢¯åº¦ï¼Œè¯„ä¼° no_gradï¼›å¼ºåˆ¶ä½¿ç”¨FP32ä»¥é˜²Convåä¼ NaN
        latents = latents / 0.18215
        latents = torch.nan_to_num(latents, nan=0.0, posinf=1e3, neginf=-1e3)
        target_dtype = torch.float32
        if self.training:
            decoded = vae.decode(latents.to(target_dtype)).sample
        else:
            with torch.no_grad():
                decoded = vae.decode(latents.to(target_dtype)).sample
        # æ•°å€¼å®‰å…¨ï¼šå»é™¤NaN/Infå¹¶é™åˆ¶åˆ°[-1,1]
        camo = torch.nan_to_num(decoded, nan=0.0, posinf=1.0, neginf=-1.0)
        camo = camo.clamp(-1, 1).to(batch_images.dtype)

        # æŒ‰æ©ç èåˆ
        masks_expanded = batch_masks.repeat(1, 3, 1, 1)
        blend_alpha = 0.85
        result = batch_images * (1.0 - blend_alpha * masks_expanded) + camo * (blend_alpha * masks_expanded)
        # æœ€ç»ˆè¿”å›å‰å†æ¬¡ç¡®ä¿æ•°å€¼ç¨³å®š
        result = torch.nan_to_num(result, nan=0.0, posinf=1.0, neginf=-1.0).clamp(-1, 1)
        return result

    def forward(self, images: torch.Tensor, masks: torch.Tensor, prompts: Optional[List[str]] = None) -> torch.Tensor:
        """nn.Module å…¼å®¹å‰å‘ï¼šå§”æ‰˜åˆ° apply_adapterã€‚"""
        return self.apply_adapter(images, masks, prompts)

    @torch.no_grad()
    def _encode_prompts(self, prompts: List[str]):
        """ç¼–ç æç¤ºè¯"""
        device = self.device
        te1 = self.txt2img_pipe.text_encoder
        te2 = self.txt2img_pipe.text_encoder_2
        tok1 = self.txt2img_pipe.tokenizer
        tok2 = self.txt2img_pipe.tokenizer_2
        
        ids1 = tok1(prompts, padding='max_length', max_length=tok1.model_max_length, 
                   truncation=True, return_tensors='pt').input_ids.to(device)
        ids2 = tok2(prompts, padding='max_length', max_length=tok2.model_max_length, 
                   truncation=True, return_tensors='pt').input_ids.to(device)
        
        out1 = te1(ids1, output_hidden_states=False, return_dict=True)
        out2 = te2(ids2, output_hidden_states=False, return_dict=True)
        
        emb1 = out1.last_hidden_state
        emb2 = out2.last_hidden_state
        pooled2 = out2.pooler_output if hasattr(out2, 'pooler_output') and out2.pooler_output is not None else emb2.mean(dim=1)
        
        return emb1, emb2, pooled2

    def compute_diffusion_loss(self, images: torch.Tensor, masks: torch.Tensor, prompts: List[str]) -> torch.Tensor:
        """è®¡ç®—æ‰©æ•£æŸå¤±"""
        if not (self.use_lora and hasattr(self, 'unet_lora')):
            return torch.tensor(0.0, device=self.device)
            
        device = self.device
        vae = self.txt2img_pipe.vae
        unet = self.unet_lora
        scheduler = self.noise_scheduler
        
        # ç¼–ç æ–‡æœ¬
        with torch.no_grad():
            cond1, cond2, pooled2 = self._encode_prompts(prompts)
            encoder_hidden_states = torch.cat([cond2, cond1], dim=-1)
            
            # ç±»å‹è½¬æ¢
            unet_dtype = next(unet.parameters()).dtype
            encoder_hidden_states = encoder_hidden_states.to(unet_dtype)
            pooled2 = pooled2.to(unet_dtype)

        # ç¼–ç å›¾åƒï¼ˆå°½é‡ä¿æŒåŠç²¾åº¦ï¼Œä¸å¼ºåˆ¶åˆ°FP32ï¼‰
        with torch.no_grad():
            img_01 = (images * 0.5) + 0.5
            img_01 = img_01.to(next(vae.parameters()).dtype)
            latents = vae.encode(img_01).latent_dist.sample() * 0.18215

        # æ·»åŠ å™ªå£°
        bsz = latents.shape[0]
        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bsz,), device=device, dtype=torch.long)
        noise = torch.randn_like(latents)
        noisy_latents = scheduler.add_noise(latents, noise, timesteps)

        # SDXLæ¡ä»¶ï¼ˆå…ˆç”¨FP32ç”Ÿæˆï¼Œå†ä¸UNetç²¾åº¦å¯¹é½ï¼‰
        target_h, target_w = model_config.image_size[1], model_config.image_size[0]
        time_ids = torch.tensor([target_h, target_w, 0, 0, target_h, target_w], 
                               device=device, dtype=torch.float32).repeat(bsz, 1)
        
        pooled2 = pooled2.to(torch.float32)
        time_ids = time_ids.to(torch.float32)

        # é¢„æµ‹ç›®æ ‡
        pred_type = getattr(scheduler.config, 'prediction_type', 'epsilon')
        if pred_type == 'epsilon':
            target = noise
        elif pred_type in ('v_prediction', 'v-prediction', 'v'):
            try:
                target = scheduler.get_velocity(latents, noise, timesteps)
            except Exception:
                target = noise
        else:
            target = noise

        # UNeté¢„æµ‹
        # ç»´æŒä¸UNetå‚æ•°ä¸€è‡´çš„ç²¾åº¦ï¼Œé¿å…æ— è°“çš„dtypeè½¬æ¢å ç”¨æ˜¾å­˜
        target_dtype = next(unet.parameters()).dtype
        pooled2 = pooled2.to(target_dtype)
        time_ids = time_ids.to(target_dtype)
        added = {'text_embeds': pooled2, 'time_ids': time_ids}

        noisy_latents = torch.nan_to_num(noisy_latents.to(target_dtype), nan=0.0, posinf=1e3, neginf=-1e3)
        encoder_hidden_states = torch.nan_to_num(encoder_hidden_states.to(target_dtype), nan=0.0, posinf=1e3, neginf=-1e3)
        
        model_pred = unet(noisy_latents, timesteps, 
                         encoder_hidden_states=encoder_hidden_states, 
                         added_cond_kwargs=added).sample
        
        # æ•°å€¼ç¨³å¥ï¼šæ¸…ç†NaN/Infå¹¶è£å‰ªæç«¯å€¼ï¼Œé¿å…PowBackward0 NaN
        model_pred = torch.nan_to_num(model_pred, nan=0.0, posinf=1e3, neginf=-1e3)
        target = torch.nan_to_num(target, nan=0.0, posinf=1e3, neginf=-1e3)
        diff = (model_pred.float() - target.float())
        diff = torch.clamp(diff, -1e3, 1e3)
        loss = torch.mean(diff * diff)
        return loss

    def generate_adaptive_camouflage_prompt(self, image: torch.Tensor, 
                                          mask: Optional[torch.Tensor] = None) -> Tuple[str, str]:
        """ç”Ÿæˆä¸¥æ ¼ç¬¦åˆè¿·å½©é£æ ¼çš„æç¤ºè¯ï¼ˆçº¹ç†ç»†èŠ‚ä¼˜åŒ–ç‰ˆï¼‰"""
        
        # 1. å¼ºåŒ–åŸºç¡€æç¤ºè¯ï¼šå¼ºè°ƒç»†èŠ‚å’ŒçœŸå®æ„Ÿ
        base_prompts = [
        "digital camouflage pattern, pixelated, 8-bit style, seamlessly blending with the background environment, matching background colors",
        "hyperrealistic pixel-level camouflage texture, intricate pixel grid, perfect environmental integration, pixelated color harmony, matches the background",
        "a pixelated pattern with crisp micro and macro pixel details, perfectly matching the environmental color and light, sharp focus, digital camo",
        "advanced digital camouflage, ultra-realistic pixelated forms, seamless environmental adaptation, 4k high resolution, blends into the background",
        "a pixel-level camouflage pattern that precisely matches the background texture and color, high quality pixelated texture, 8-bit camo style"
        ]
        
        # 2. ä¸°å¯Œé£æ ¼åº“ï¼šåŠ å…¥æ›´å¤šå…³äºè´¨æ„Ÿã€å…‰å½±å’Œæ¸…æ™°åº¦çš„å…³é”®è¯
        style_elements = [
            "intricate textures", "sharp focus", "hyperrealistic", "professional photography",
            "natural lighting", "subtle shadows", "UHD", "trending on ArtStation",
            "filmic", "physically-based rendering", "cinematic lighting",
            "environmental color matching", "seamless background integration", 
            "organic shape distribution", "tactical effectiveness"
        ]
        

        
        # ç¡®ä¿æ¯æ¬¡éƒ½æœ‰è´¨é‡ä¿è¯è¯ç¼€
        quality_boilerplate = "(best quality, 4k, highres, masterpiece:1.2)"
        
        positive_prompt = f"{base_prompts}, {', '.join(style_elements)}, {quality_boilerplate}"
        
        # ä»configåŠ è½½æ•´åˆåçš„è´Ÿé¢æç¤ºè¯
        from config import NEGATIVE_PROMPTS_CONFIG
        
        # å¢å¼ºè´Ÿé¢æç¤ºï¼Œé¿å…æ¨¡ç³Šå’Œä½è´¨é‡
        additional_negative = [
            "blurry, soft focus, out of focus, low quality, jpeg artifacts",
            "smooth, plain, flat texture, glossy, plastic look",
            "painting, drawing, cartoon, anime, illustration",
            "watermark, text, signature, logo"
        ]

        # ç»„åˆæ‰€æœ‰è´Ÿé¢æç¤ºï¼Œå¹¶å»é‡
        all_negative_elements = (NEGATIVE_PROMPTS_CONFIG['base'] +
                                 NEGATIVE_PROMPTS_CONFIG['color_strict'] +
                                 additional_negative)
        negative_prompt = ", ".join(list(dict.fromkeys(all_negative_elements))) # ä½¿ç”¨dict.fromkeyså»é‡å¹¶ä¿æŒé¡ºåº
        
        return positive_prompt, negative_prompt

    def _tensor_to_pil(self, tensor: torch.Tensor, is_mask: bool = False) -> Image.Image:
        """
        å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ (å®Œæ•´åŠŸèƒ½ç‰ˆ)
        - æ”¯æŒæ™®é€šå›¾åƒå’Œå•é€šé“æ©ç 
        """
        if is_mask:
            # --- æ©ç å¤„ç†é€»è¾‘ ---
            # æ©ç å¼ é‡é€šå¸¸æ˜¯ [H, W] æˆ– [1, H, W]ï¼ŒèŒƒå›´ [0, 1]
            if tensor.dim() == 3:  # å¦‚æœæ˜¯ [1, H, W]
                tensor = tensor.squeeze(0)  # é™ç»´åˆ° [H, W]

            # å°† [0, 1] èŒƒå›´çš„å¼ é‡è½¬æ¢ä¸º [0, 255] çš„numpyæ•°ç»„
            np_array = (tensor.cpu().numpy() * 255).astype(np.uint8)
            # ä»æ•°ç»„åˆ›å»ºç°åº¦å›¾ ('L' mode)
            return Image.fromarray(np_array, mode='L')
        else:
            # --- æ™®é€šå›¾åƒå¤„ç†é€»è¾‘ ---
            # åå½’ä¸€åŒ–ï¼Œä» [-1, 1] èŒƒå›´è½¬æ¢åˆ° [0, 1]
            tensor = (tensor + 1.0) / 2.0
            tensor = torch.clamp(tensor, 0, 1)

            # è½¬æ¢ä¸º [H, W, C] çš„numpyæ•°ç»„å¹¶è°ƒæ•´æ•°å€¼èŒƒå›´
            img_array = tensor.permute(1, 2, 0).cpu().numpy()
            img_array = (img_array * 255).astype(np.uint8)
            # ä»æ•°ç»„åˆ›å»ºå½©è‰²å›¾ ('RGB' mode)
            return Image.fromarray(img_array, mode='RGB')

    def _select_best_reference_by_clip(self, image_pil: Image.Image, mask_pil: Image.Image):
        """ä½¿ç”¨CLIPé€‰æ‹©æœ€ä½³å‚è€ƒå›¾åƒï¼ˆåŸºäºèƒŒæ™¯åŒºåŸŸï¼‰"""
        if not self.reference_db or self.clip_model is None:
            return {'image': image_pil, 'tensor': None}
        
        # åŸºäºèƒŒæ™¯åŒºåŸŸçš„å›¾åƒï¼ˆæ©ç å¤–ï¼‰
        try:
            if mask_pil is not None:
                # èƒŒæ™¯ = åŸå›¾ * (1 - mask)
                inv_mask = Image.eval(mask_pil.convert('L'), lambda p: 255 - p)
                bg_image = Image.composite(image_pil, Image.new('RGB', image_pil.size, (0,0,0)), inv_mask)
            else:
                bg_image = image_pil
        except Exception:
            bg_image = image_pil
        
        try:
            if self._clip_type == 'huggingface':
                inputs = self.clip_processor(images=bg_image, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    image_features = self.clip_model.get_image_features(**inputs)
                
                best_similarity = -1
                best_ref = self.reference_db[0]
                
                for ref in self.reference_db[:5]:
                    ref_inputs = self.clip_processor(images=ref['image'], return_tensors="pt").to(self.device)
                    with torch.no_grad():
                        ref_features = self.clip_model.get_image_features(**ref_inputs)
                    
                    similarity = torch.cosine_similarity(image_features, ref_features).item()
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_ref = ref
                
                return best_ref
                
            elif self._clip_type == 'openclip':
                image_tensor = self.clip_processor(bg_image).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    image_features = self.clip_model.encode_image(image_tensor)
                
                best_similarity = -1
                best_ref = self.reference_db[0]
                
                for ref in self.reference_db[:5]:
                    ref_tensor = self.clip_processor(ref['image']).unsqueeze(0).to(self.device)
                    with torch.no_grad():
                        ref_features = self.clip_model.encode_image(ref_tensor)
                    
                    similarity = torch.cosine_similarity(image_features, ref_features).item()
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_ref = ref
                
                return best_ref
            else:
                import random
                return random.choice(self.reference_db) if self.reference_db else {'image': image_pil, 'tensor': None}
        except Exception as e:
            print(f"âš ï¸ CLIPå‚è€ƒé€‰æ‹©å¤±è´¥: {e}")
            return self.reference_db[0] if self.reference_db else {'image': image_pil, 'tensor': None}

    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹"""
        state_dict = {}
        
        if hasattr(self, 'adapter'):
            state_dict['adapter_state_dict'] = self.adapter.state_dict()
        
        if hasattr(self, 'feature_projector'):
            state_dict['feature_projector_state_dict'] = self.feature_projector.state_dict()
        
        if hasattr(self, 'unet_lora'):
            state_dict['unet_lora_state_dict'] = self.unet_lora.state_dict()
        
        torch.save(state_dict, save_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

    def load_model(self, load_path: str):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(load_path):
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
            return
        
        state_dict = torch.load(load_path, map_location=self.device)
        
        # ç¡®ä¿é€‚é…å™¨å­˜åœ¨
        if not hasattr(self, 'adapter'):
            self.get_trainable_parameters()
        
        # åŠ è½½çŠ¶æ€
        if 'adapter_state_dict' in state_dict:
            self.adapter.load_state_dict(state_dict['adapter_state_dict'])
        
        if 'feature_projector_state_dict' in state_dict:
            self.feature_projector.load_state_dict(state_dict['feature_projector_state_dict'])
        
        if 'unet_lora_state_dict' in state_dict and hasattr(self, 'unet_lora'):
            self.unet_lora.load_state_dict(state_dict['unet_lora_state_dict'])
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {load_path}")
