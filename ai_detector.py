"""
AIæ£€æµ‹å™¨æ¨¡å— - å¤šæ¨¡æ€è¿·å½©æ£€æµ‹ç³»ç»Ÿ
åŒ…å«YOLOv8ç›®æ ‡æ£€æµ‹ã€CLIPè¯­ä¹‰ç†è§£ã€EfficientNetåˆ†ç±»ç­‰å¤šä¸ªæ£€æµ‹å™¨
"""
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from PIL import Image
import clip
import cv2
from ultralytics import YOLO
from torchvision import transforms
import timm
from typing import List, Dict, Tuple, Optional, Union

from config import model_config, eval_config, training_config


class YOLODetector(nn.Module):
    """YOLOç›®æ ‡æ£€æµ‹å™¨"""
    
    def __init__(self, model_name: str = "yolov8x.pt", device: str = "cuda"):
        """
        åˆå§‹åŒ–YOLOæ£€æµ‹å™¨
        
        Args:
            model_name: YOLOæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()
        self.device = device
        
        # å°è¯•ä½¿ç”¨æœ¬åœ°YOLOæ¨¡å‹
        local_yolo_path = "models/yolov8x.pt"
        
        if os.path.exists(local_yolo_path):
            try:
                print(f"å°è¯•åŠ è½½æœ¬åœ°YOLOæ¨¡å‹: {local_yolo_path}")
                self.model = YOLO(local_yolo_path)
                print("âœ… æˆåŠŸåŠ è½½æœ¬åœ°YOLOæ¨¡å‹")
            except Exception as e:
                raise RuntimeError(f"æœ¬åœ°YOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}. è¯·ç¡®ä¿æ–‡ä»¶å®Œæ•´: {local_yolo_path}")
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ¬åœ°YOLOæƒé‡: {local_yolo_path}ã€‚è¯·å°† yolov8x.pt æ”¾åˆ° models/ ç›®å½•ä¸‹ã€‚")
        
        self.model.to(device)
        
        # æ£€æµ‹é˜ˆå€¼
        self.confidence_threshold = eval_config.detection_confidence
        self.iou_threshold = eval_config.detection_iou_threshold
        
        print(f"YOLOæ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
    
    def forward(self, images: Union[torch.Tensor, List[Image.Image]], masks: Optional[Union[torch.Tensor, List[Image.Image]]] = None) -> Dict:
        """
        å‰å‘æ¨ç†
        
        Args:
            images: è¾“å…¥å›¾åƒ
            masks: å¯é€‰æ©ç ï¼ˆåªæ£€æµ‹æ©ç åŒºåŸŸï¼‰ã€‚æ”¯æŒ[B,1,H,W]å¼ é‡æˆ–PILåˆ—è¡¨
            
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        if isinstance(images, torch.Tensor):
            # å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒåˆ—è¡¨
            images = self._tensor_to_pil_list(images)
        
        mask_list: Optional[List[Image.Image]] = None
        if masks is not None:
            if isinstance(masks, torch.Tensor):
                mask_list = self._tensor_mask_to_pil_list(masks)
            else:
                mask_list = [m.convert('L') for m in masks]
        
        # è‹¥ç»™å®šæ©ç ï¼Œåˆ™ä»…å¯¹æ©ç åŒºåŸŸè¿›è¡Œæ£€æµ‹
        if mask_list is not None:
            inference_images: List[Image.Image] = []
            for img, m in zip(images, mask_list):
                black = Image.new('RGB', img.size, (0, 0, 0))
                masked_img = Image.composite(img, black, m)
                inference_images.append(masked_img)
        else:
            inference_images = images
        
        # YOLOæ¨ç†
        results = self.model(
            inference_images,
            conf=self.confidence_threshold,
            iou=self.iou_threshold,
            verbose=False
        )
        
        # è§£æç»“æœ
        detection_results = {
            'boxes': [],         # æ£€æµ‹æ¡†åæ ‡ [x1, y1, x2, y2]
            'scores': [],        # æ£€æµ‹ç½®ä¿¡åº¦ [0.0-1.0]
            'classes': [],       # æ£€æµ‹ç±»åˆ«ID [0=person, 1=bicycle, ...]
            'detected': []       # æ˜¯å¦æ£€æµ‹åˆ° [True/False]
        }
        
        for result in results:
            if result.boxes is not None and len(result.boxes) > 0:
                boxes = result.boxes.xyxy.cpu().numpy()
                scores = result.boxes.conf.cpu().numpy()
                classes = result.boxes.cls.cpu().numpy()
                
                detection_results['boxes'].append(boxes)
                detection_results['scores'].append(scores)
                detection_results['classes'].append(classes)
                detection_results['detected'].append(len(boxes) > 0)
            else:
                detection_results['boxes'].append(np.array([]))     
                detection_results['scores'].append(np.array([]))
                detection_results['classes'].append(np.array([]))
                detection_results['detected'].append(False)
        
        return detection_results
    
    def _tensor_to_pil_list(self, tensor: torch.Tensor) -> List[Image.Image]:
        """å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒåˆ—è¡¨ï¼ˆæ¢¯åº¦å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        # åˆ†ç¦»æ¢¯åº¦ä»¥é¿å…è®¡ç®—å›¾é—®é¢˜
        tensor = tensor.detach()
        
        # åå½’ä¸€åŒ–
        tensor = (tensor * 0.5) + 0.5
        tensor = torch.clamp(tensor, 0, 1)
        
        pil_images = []
        for i in range(tensor.shape[0]):
            img = tensor[i]
            # [C,H,W]ï¼Œè‹¥æ˜¯3é€šé“æŒ‰RGBï¼›è‹¥æ˜¯1é€šé“åˆ™æŒ¤æ‰é€šé“ç»´
            if img.shape[0] == 3:
                arr = (img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                pil_images.append(Image.fromarray(arr, mode='RGB'))
            elif img.shape[0] == 1:
                arr = (img[0].cpu().numpy() * 255).astype(np.uint8)
                pil_images.append(Image.fromarray(arr, mode='L'))
            else:
                # å…œåº•ï¼šå°è¯•æŒ‰RGB
                arr = (img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                pil_images.append(Image.fromarray(arr))
        
        return pil_images

    def _tensor_mask_to_pil_list(self, tensor: torch.Tensor) -> List[Image.Image]:
        """æ©ç ä¸“ç”¨ï¼šå°† [B,1,H,W] æˆ– [B,H,W] å¼ é‡è½¬ä¸ºPIL Lå›¾åƒåˆ—è¡¨"""
        t = tensor.detach()
        if t.dim() == 3:
            # [B,H,W] -> [B,1,H,W]
            t = t.unsqueeze(1)
        # ç¡®ä¿ä¸º[0,1]
        if t.min() < 0 or t.max() > 1:
            t = t.clamp(0, 1)
        masks: List[Image.Image] = []
        for i in range(t.shape[0]):
            arr = (t[i, 0].cpu().numpy() * 255).astype(np.uint8)
            masks.append(Image.fromarray(arr, mode='L'))
        return masks
    
    def compute_detection_loss(self, images: torch.Tensor, target_detected: bool = False) -> torch.Tensor:
        """
        è®¡ç®—æ£€æµ‹æŸå¤±ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼‰
        
        Args:
            images: è¾“å…¥å›¾åƒå¼ é‡
            target_detected: ç›®æ ‡æ£€æµ‹çŠ¶æ€ï¼ˆFalseè¡¨ç¤ºå¸Œæœ›ä¸è¢«æ£€æµ‹åˆ°ï¼‰
            
        Returns:
            æ£€æµ‹æŸå¤±
        """
        # ä½¿ç”¨çœŸå®AIæ£€æµ‹å™¨è®¡ç®—æŸå¤±ï¼ˆæ¢¯åº¦å®‰å…¨ç‰ˆæœ¬ï¼‰
        try:
            # åˆ†ç¦»æ¢¯åº¦è¿›è¡ŒAIæ£€æµ‹ï¼Œä½†ä¿ç•™æ£€æµ‹ç»“æœç”¨äºæŸå¤±è®¡ç®—
            with torch.no_grad():
                results = self.forward(images)
            
            batch_size = len(results['detected'])
            
            # è®¡ç®—åŸºäºçœŸå®AIæ£€æµ‹çš„æŸå¤±
            detection_scores = []
            for i in range(batch_size):
                detected = results['detected'][i]
                confidence = results['confidence'][i] if 'confidence' in results else 0.5
                
                # å°†æ£€æµ‹ç»“æœè½¬æ¢ä¸ºè¿ç»­å€¼ç”¨äºæ¢¯åº¦è®¡ç®—
                if detected:
                    score = confidence  # æ£€æµ‹åˆ°ï¼Œç½®ä¿¡åº¦è¶Šé«˜è¶Š"è¢«æ£€æµ‹"
                else:
                    score = 0.0  # æœªæ£€æµ‹åˆ°
                
                detection_scores.append(score)
            
            # è½¬æ¢ä¸ºå¼ é‡
            detection_tensor = torch.tensor(detection_scores, device=images.device, dtype=torch.float32)
            
            # æ ¹æ®ç›®æ ‡è®¡ç®—æŸå¤±
            if target_detected:
                # å¸Œæœ›è¢«æ£€æµ‹åˆ°ï¼šæœ€å¤§åŒ–æ£€æµ‹åˆ†æ•°
                loss = -detection_tensor.mean()
            else:
                # å¸Œæœ›ä¸è¢«æ£€æµ‹åˆ°ï¼ˆè¿·å½©ç›®æ ‡ï¼‰ï¼šæœ€å°åŒ–æ£€æµ‹åˆ†æ•°
                loss = detection_tensor.mean()
            
            # ä¸ºäº†ä¿æŒæ¢¯åº¦æµï¼Œæ·»åŠ ä¸€ä¸ªä¸è¾“å…¥å›¾åƒç›¸å…³çš„å¾®å°é¡¹
            # è¿™ç¡®ä¿æŸå¤±å¯¹è¾“å…¥å›¾åƒæœ‰ä¾èµ–æ€§
            image_variance = torch.var(images, dim=[1,2,3]).mean()
            loss = loss + 0.001 * image_variance  # å¾®å°çš„å›¾åƒç›¸å…³é¡¹
            
            return loss
            
        except Exception as e:
            print(f"AIæ£€æµ‹æŸå¤±è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºå›¾åƒç»Ÿè®¡çš„ç®€å•æŸå¤±
            image_mean = torch.mean(images, dim=[1,2,3])
            if target_detected:
                return -torch.var(image_mean)  # å¸Œæœ›è¢«æ£€æµ‹ï¼šå¢åŠ å›¾åƒå·®å¼‚
            else:
                return torch.var(image_mean)   # å¸Œæœ›ä¸è¢«æ£€æµ‹ï¼šå‡å°‘å›¾åƒå·®å¼‚

class CLIPDetector(nn.Module):
    """CLIPè¯­ä¹‰æ£€æµ‹å™¨"""
    
    def __init__(self, model_name: str = "ViT-L/14", device: str = "cuda"):
        """
        åˆå§‹åŒ–CLIPæ£€æµ‹å™¨
        
        Args:
            model_name: CLIPæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()
        self.device = device
        
        # ç»Ÿä¸€çš„tokenizeå‡½æ•°å ä½
        self.tokenize = None
        
        # 1) ä¼˜å…ˆå°è¯• openai/CLIP æ¥å£ï¼ˆè‹¥ç¯å¢ƒä¸­çš„ clip æ‹¥æœ‰ load/tokenizeï¼‰
        use_openai_clip = hasattr(clip, "load") and hasattr(clip, "tokenize")
        local_clip_path = "models/ViT-L-14.pt"
        if use_openai_clip:
            try:
                if os.path.exists(local_clip_path):
                    print(f"å°è¯•åŠ è½½æœ¬åœ°CLIPæ¨¡å‹(OpenAIåç«¯): {local_clip_path}")
                    self.model, self.preprocess = clip.load(model_name, device=device, download_root="models")
                else:
                    raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ¬åœ°CLIPæƒé‡: {local_clip_path}ã€‚è¯·å°† ViT-L-14.pt æ”¾åˆ° models/ ç›®å½•ä¸‹ï¼Œæˆ–ä½¿ç”¨ open_clip å›é€€ã€‚")
                self.tokenize = clip.tokenize
                print("âœ… æˆåŠŸåŠ è½½CLIP(OpenAI)")
            except Exception as e:
                print(f"âš ï¸ OpenAI CLIP åŠ è½½å¤±è´¥: {e}")
                use_openai_clip = False
        
        # 2) å›é€€åˆ° open_clipï¼ˆæ— éœ€ä»GitHubå®‰è£…ï¼Œå¯pipå®‰è£… open_clip_torchï¼›æ”¯æŒæœ¬åœ° .pt æƒé‡ï¼‰
        if not use_openai_clip:
            try:
                import open_clip
                print("å°è¯•åŠ è½½CLIP(OpenCLIPåç«¯)...")
                if os.path.exists(local_clip_path):
                    model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained=local_clip_path)
                else:
                    raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ¬åœ°CLIPæƒé‡: {local_clip_path}ã€‚è¯·å°† ViT-L-14.pt æ”¾åˆ° models/ ç›®å½•ä¸‹ã€‚")
                self.model = model.to(device)
                self.preprocess = preprocess
                self.tokenize = open_clip.tokenize
                print("âœ… æˆåŠŸåŠ è½½CLIP(OpenCLIP)")
            except Exception as e:
                raise RuntimeError("æ— æ³•åŠ è½½CLIPï¼šæ—¢æ²¡æœ‰å¯ç”¨çš„ openai/CLIPï¼Œä¹Ÿæ— æ³•å›é€€åˆ° open_clipã€‚è¯·å®‰è£… open_clip_torch æˆ–ä¿®æ­£ clip åŒ…ã€‚") from e
        
        # å®šä¹‰æ£€æµ‹æ–‡æœ¬
        self.detection_texts = [
            "a person hiding in camouflage",
            "military camouflage pattern", 
            "person in military uniform",
            "camouflaged object",
            "hidden person",
            "person blending with background",
            "military personnel in camouflage",
            "concealed person"
        ]
        
        # ä¸å†éœ€è¦å›ºå®šçš„normal_texts
        # self.normal_texts = [ ... ]
        
        # åªé¢„ç¼–ç detection_texts
        with torch.no_grad():
            camouflage_tokens = self.tokenize(self.detection_texts).to(device)
            self.camouflage_features = self.model.encode_text(camouflage_tokens)
            self.camouflage_features /= self.camouflage_features.norm(dim=-1, keepdim=True)
        
        print(f"CLIPæ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
    
    def forward(self, images: Union[torch.Tensor, List[Image.Image]]) -> Dict:
        """
        å‰å‘æ¨ç† (æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬ä¸æ¥æ”¶èƒŒæ™¯å›¾ï¼Œä¸»è¦ç”¨äºæ— èƒŒæ™¯å‚è€ƒçš„å¿«é€Ÿè¯„ä¼°)
        
        Args:
            images: è¾“å…¥å›¾åƒ
            
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        if isinstance(images, torch.Tensor):
            images = self._tensor_to_pil_list(images)
        
        # é¢„å¤„ç†å›¾åƒ
        image_tensors = []
        for img in images:
            image_tensors.append(self.preprocess(img))
        
        image_batch = torch.stack(image_tensors).to(self.device)
        
        # ç¼–ç å›¾åƒ
        with torch.no_grad():
            image_features = self.model.encode_image(image_batch)
            image_features /= image_features.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ä¸è¿·å½©çš„ç›¸ä¼¼åº¦
        camouflage_similarities = torch.matmul(image_features, self.camouflage_features.T)
        max_camouflage_sim = camouflage_similarities.max(dim=-1)[0]
        
        # ç”±äºæ²¡æœ‰èƒŒæ™¯å‚è€ƒï¼Œè¿™é‡Œçš„æ£€æµ‹é€»è¾‘ç®€åŒ–
        # å®é™…çš„å¯¹æŠ—æŸå¤±å°†ä½¿ç”¨æ›´å¤æ‚çš„é€»è¾‘
        detection_threshold = 0.25 # è®¾å®šä¸€ä¸ªç»å¯¹é˜ˆå€¼
        detected = max_camouflage_sim > detection_threshold
        
        return {
            'camouflage_similarity': max_camouflage_sim,
            'normal_similarity': torch.zeros_like(max_camouflage_sim), # å ä½
            'detected': detected,
            'confidence': max_camouflage_sim
        }
    
    def _tensor_to_pil_list(self, tensor: torch.Tensor) -> List[Image.Image]:
        """å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒåˆ—è¡¨ï¼ˆæ¢¯åº¦å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        # åˆ†ç¦»æ¢¯åº¦ä»¥é¿å…è®¡ç®—å›¾é—®é¢˜
        tensor = tensor.detach()
        
        # åå½’ä¸€åŒ–
        tensor = (tensor * 0.5) + 0.5
        tensor = torch.clamp(tensor, 0, 1)
        
        pil_images = []
        for i in range(tensor.shape[0]):
            img_array = (tensor[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            pil_images.append(Image.fromarray(img_array))
        
        return pil_images
    
    def compute_detection_loss(self, images: torch.Tensor, target_detected: bool = False) -> torch.Tensor:
        """è®¡ç®—CLIPæ£€æµ‹æŸå¤±"""
        # æ³¨æ„ï¼šè¯¥æ–¹æ³•ä»…ç”¨äºè¯„ä¼°ä¸è®°å½•ï¼Œä¸å‚ä¸åä¼ 
        # ä¸ä½¿ç”¨detachï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶ä¿ç•™å¯å¾®è·¯å¾„ï¼›ä½†è°ƒç”¨è¯¥æ–¹æ³•çš„ä¸Šå±‚é€šå¸¸åŒ…è£¹no_grad()
        results = self.forward(images)
        
        confidence = results['confidence']
        
        if target_detected:
            # å¸Œæœ›è¢«æ£€æµ‹åˆ°ï¼šæœ€å¤§åŒ–è¿·å½©ç›¸ä¼¼åº¦
            loss = -confidence.mean()
        else:
            # å¸Œæœ›ä¸è¢«æ£€æµ‹åˆ°ï¼šæœ€å°åŒ–è¿·å½©ç›¸ä¼¼åº¦
            loss = torch.relu(confidence + 0.1).mean()  # ReLUç¡®ä¿åªæƒ©ç½šæ­£çš„confidence
        
        return loss

    def compute_detection_loss_diff(self, 
                                  images: torch.Tensor, 
                                  background_images: torch.Tensor,
                                  target_detected: bool = False) -> torch.Tensor:
        """
        å¯å¾®åˆ†çš„CLIPæ£€æµ‹æŸå¤±ï¼Œä½¿ç”¨çœŸå®çš„èƒŒæ™¯å›¾åƒä½œä¸ºåŸºå‡†ã€‚
        """
        def encode_image_for_clip(img_tensor):
            # å°†å›¾åƒä»[-1,1]æ˜ å°„åˆ°[0,1]
            x = (img_tensor * 0.5) + 0.5
            # è°ƒæ•´åˆ°CLIPè¾“å…¥åˆ†è¾¨ç‡
            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)
            # æŒ‰CLIPè§„èŒƒåŒ–
            clip_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=x.device).view(1,3,1,1)
            clip_std  = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=x.device).view(1,3,1,1)
            x = (x - clip_mean) / clip_std
            # å›¾åƒç¼–ç ï¼ˆä¿æŒå¯å¾®ï¼‰
            features = self.model.encode_image(x)
            return features / features.norm(dim=-1, keepdim=True)

        # ç¼–ç ç”Ÿæˆçš„å›¾åƒå’ŒèƒŒæ™¯å›¾åƒ
        generated_features = encode_image_for_clip(images)
        background_features = encode_image_for_clip(background_images)

        # è®¡ç®—ç”Ÿæˆå›¾ä¸â€œè¿·å½©â€æ–‡æœ¬çš„ç›¸ä¼¼åº¦
        camouflage_similarities = torch.matmul(generated_features, self.camouflage_features.T)
        max_camouflage_sim = camouflage_similarities.max(dim=-1)[0]

        # è®¡ç®—ç”Ÿæˆå›¾ä¸â€œçœŸå®èƒŒæ™¯â€çš„ç›¸ä¼¼åº¦ (é€å…ƒç´ )
        # background_features.unsqueeze(1) -> [B, 1, D]
        # generated_features.unsqueeze(2) -> [B, D, 1]
        # bmm -> [B, 1, 1] -> squeeze -> [B]
        background_similarity = torch.bmm(generated_features.unsqueeze(1), background_features.unsqueeze(2)).squeeze()

        # å¯¹æŠ—ç›®æ ‡ï¼šæœ€å°åŒ–(è¿·å½©ç›¸ä¼¼åº¦ - èƒŒæ™¯ç›¸ä¼¼åº¦)
        # å³ï¼Œè®©è¿·å½©ç›¸ä¼¼åº¦å°½å¯èƒ½ä½ï¼ŒèƒŒæ™¯ç›¸ä¼¼åº¦å°½å¯èƒ½é«˜
        confidence = max_camouflage_sim - background_similarity
        
        if target_detected:
            # è¿™ä¸ªåˆ†æ”¯åœ¨å¯¹æŠ—è®­ç»ƒä¸­é€šå¸¸ä¸ç”¨
            loss = -confidence.mean()
        else:
            # æˆ‘ä»¬å¸Œæœ›confidenceæ˜¯è´Ÿæ•°ï¼ˆèƒŒæ™¯ç›¸ä¼¼åº¦ > è¿·å½©ç›¸ä¼¼åº¦ï¼‰
            # æ‰€ä»¥æˆ‘ä»¬æƒ©ç½šæ‰€æœ‰æ­£çš„confidenceå€¼
            loss = torch.relu(confidence).mean()
        
        return loss

    def compute_adversarial_loss(self, 
                                 images: torch.Tensor, 
                                 background_images: torch.Tensor,
                                 target_detected: bool = False) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """ç»Ÿä¸€æ¥å£ï¼šè¿”å› (loss, details)"""
        loss = self.compute_detection_loss_diff(images, background_images, target_detected=target_detected)
        details = {
            'clip_semantic_loss': loss.detach()
        }
        return loss, details

class EfficientNetClassifier(nn.Module):
    """EfficientNetåˆ†ç±»å™¨"""
    
    def __init__(self, 
                 model_name: str = "efficientnet_b4",  # æ”¹ç”¨æ›´å¸¸è§çš„b4
                 num_classes: int = 2,  # 0: æ— è¿·å½©, 1: æœ‰è¿·å½©
                 pretrained: bool = True,
                 device: str = "cuda"):
        """
        åˆå§‹åŒ–EfficientNetåˆ†ç±»å™¨
        
        Args:
            model_name: EfficientNetæ¨¡å‹åç§°
            num_classes: åˆ†ç±»æ•°é‡
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()
        self.device = device
        self.num_classes = num_classes
        
        # ç»Ÿä¸€ä½¿ç”¨timmåŠ è½½æ¨¡å‹ï¼Œé¿å…transformersçš„torch.loadç‰ˆæœ¬é™åˆ¶é—®é¢˜
        local_model_path = "models/efficientnet-b4"
        model_name_to_load = model_config.efficientnet_model or "efficientnet_b4"
        model_loaded = False
        
        # ç¦»çº¿æ¨¡å¼ä¸‹ç¦ç”¨é¢„è®­ç»ƒæƒé‡ä¸‹è½½
        offline = os.environ.get("HF_HUB_OFFLINE", "0") == "1" or os.environ.get("TRANSFORMERS_OFFLINE", "0") == "1"
        use_pretrained = pretrained and (not offline)
        if offline and pretrained:
            print("ğŸŒ å·²å¯ç”¨ç¦»çº¿æ¨¡å¼(HF_HUB_OFFLINE/TRANSFORMERS_OFFLINE)ï¼Œç¦ç”¨timmé¢„è®­ç»ƒæƒé‡ä¸‹è½½ï¼Œæ”¹ä¸ºéšæœºåˆå§‹åŒ–ã€‚")

        try:
            print(f"å°è¯•ä½¿ç”¨timmåŠ è½½æ¨¡å‹: {model_name_to_load}")
            self.backbone = timm.create_model(
                model_name_to_load, 
                pretrained=use_pretrained,
                num_classes=num_classes
            )
            init_msg = "(é¢„è®­ç»ƒæƒé‡)" if use_pretrained else "(éšæœºåˆå§‹åŒ–)"
            print(f"âœ… æˆåŠŸåŠ è½½ {model_name_to_load} {init_msg}")
            model_loaded = True
            self.use_transformers = False # ç»Ÿä¸€è®¾ç½®ä¸ºFalse
            
        except Exception as e:
            print(f"âš ï¸ ä½¿ç”¨timmåŠ è½½ {model_name_to_load} å¤±è´¥: {e}")
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå¯ä»¥æ·»åŠ å¤‡é€‰æ–¹æ¡ˆï¼Œä¾‹å¦‚ResNet
            try:
                print("å°è¯•åŠ è½½å¤‡é€‰æ¨¡å‹: resnet50")
                self.backbone = timm.create_model(
                    "resnet50",
                    pretrained=use_pretrained,
                    num_classes=num_classes
                )
                init_msg = "(é¢„è®­ç»ƒæƒé‡)" if use_pretrained else "(éšæœºåˆå§‹åŒ–)"
                print(f"âœ… æˆåŠŸåŠ è½½ resnet50 {init_msg}")
                model_loaded = True
                self.use_transformers = False
            except Exception as fallback_e:
                print(f"âš ï¸ å¤‡é€‰æ¨¡å‹resnet50åŠ è½½å¤±è´¥: {fallback_e}")

        if not model_loaded:
            # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ ResNet18 éšæœºåˆå§‹åŒ–
            print("âš ï¸  æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨ ResNet18 éšæœºåˆå§‹åŒ–")
            self.backbone = timm.create_model(
                "resnet18", 
                pretrained=False,
                num_classes=num_classes
            )
            self.use_transformers = False
        
        self.backbone.to(device)
        
        # æ£€æŸ¥å¹¶æŠ¥å‘Šæ¨¡å‹çŠ¶æ€
        self._report_model_status()
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),  # å‡å°è¾“å…¥å°ºå¯¸èŠ‚çœæ˜¾å­˜
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        print(f"EfficientNetåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
    
    def _report_model_status(self):
        """æŠ¥å‘Šæ¨¡å‹çŠ¶æ€å’Œé…ç½®ä¿¡æ¯"""
        try:
            if hasattr(self, 'use_transformers') and self.use_transformers:
                # Transformersæ¨¡å‹
                config = self.backbone.config
                print(f"ğŸ“‹ æ¨¡å‹æ¶æ„: {config.architectures[0] if hasattr(config, 'architectures') else 'EfficientNet'}")
                print(f"ğŸ¯ åˆ†ç±»ç±»åˆ«: {config.num_labels}")
                print(f"ğŸ”§ æ¨¡å‹çŠ¶æ€: é¢„è®­ç»ƒä¸»å¹² + è‡ªå®šä¹‰åˆ†ç±»å™¨")
                
                # æ£€æŸ¥å“ªäº›å±‚è¢«é‡æ–°åˆå§‹åŒ–
                classifier_layer = None
                for name, module in self.backbone.named_modules():
                    if 'classifier' in name and hasattr(module, 'weight'):
                        classifier_layer = name
                        break
                
                if classifier_layer and self.num_classes != 1000:
                    print(f"ğŸ”„ é‡æ–°åˆå§‹åŒ–å±‚: {classifier_layer}")
                    print(f"ğŸ’¡ è¿™æ˜¯Transfer Learningçš„æ­£å¸¸è¡Œä¸º")
            else:
                # TIMMæ¨¡å‹
                print(f"ğŸ“‹ æ¨¡å‹æ¶æ„: {self.backbone.__class__.__name__}")
                print(f"ğŸ¯ åˆ†ç±»ç±»åˆ«: {self.num_classes}")
                print(f"ğŸ”§ æ¨¡å‹çŠ¶æ€: TIMMé¢„è®­ç»ƒæ¨¡å‹")
                
        except Exception as e:
            print(f"ğŸ“Š æ¨¡å‹çŠ¶æ€æ£€æŸ¥è·³è¿‡: {e}")
    
    def forward(self, images: torch.Tensor, masks: Optional[torch.Tensor] = None) -> Dict:
        """
        å‰å‘æ¨ç†
        
        Args:
            images: è¾“å…¥å›¾åƒå¼ é‡ [B, C, H, W]
            
        Returns:
            åˆ†ç±»ç»“æœå­—å…¸
        """
        # é¢„å¤„ç†å›¾åƒ
        if images.shape[-1] != 224 or images.shape[-2] != 224:
            images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)
        
        # åå½’ä¸€åŒ–åé‡æ–°å½’ä¸€åŒ–åˆ°ImageNetæ ‡å‡†
        images = (images * 0.5) + 0.5  # åå½’ä¸€åŒ–åˆ°[0,1]
        
        if hasattr(self, 'use_transformers') and self.use_transformers:
            # ä½¿ç”¨transformersæ¨¡å‹
            # è½¬æ¢ä¸ºPILå›¾åƒåˆ—è¡¨ç»™processorå¤„ç†ï¼ˆæ¢¯åº¦å®‰å…¨ç‰ˆæœ¬ï¼‰
            pil_images = []
            for i in range(images.shape[0]):
                img_array = (images[i].detach().permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                pil_images.append(Image.fromarray(img_array))
            
            # ä½¿ç”¨processorå¤„ç†
            inputs = self.processor(pil_images, return_tensors="pt").to(self.device)
            
            # å‰å‘æ¨ç†
            with torch.no_grad():
                outputs = self.backbone(**inputs)
                logits = outputs.logits
        else:
            # ä½¿ç”¨timmæ¨¡å‹
            images = self.transform(images)
            logits = self.backbone(images)
        
        probs = torch.softmax(logits, dim=-1)
        
        # è·å–é¢„æµ‹
        predicted_classes = torch.argmax(probs, dim=-1)
        confidence = torch.max(probs, dim=-1)[0]
        
        return {
            'logits': logits,
            'probabilities': probs,
            'predicted_classes': predicted_classes,
            'confidence': confidence,
            'detected': predicted_classes == 1  # ç±»åˆ«1è¡¨ç¤ºæ£€æµ‹åˆ°è¿·å½©
        }
    
    def compute_detection_loss(self, images: torch.Tensor, target_detected: bool = False) -> torch.Tensor:
        """è®¡ç®—åˆ†ç±»æ£€æµ‹æŸå¤±"""
        results = self.forward(images)
        
        # æ„å»ºç›®æ ‡æ ‡ç­¾
        batch_size = images.shape[0]
        target_labels = torch.full((batch_size,), 1 if target_detected else 0, 
                                 dtype=torch.long, device=images.device)
        
        # äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(results['logits'], target_labels)
        
        return loss

    def compute_detection_loss_diff(self, images: torch.Tensor, target_detected: bool = False) -> torch.Tensor:
        """å¯å¾®åˆ†åˆ†ç±»æŸå¤±ï¼šçº¯å¼ é‡è·¯å¾„ï¼Œä¸ç»PILä¸no_gradã€‚"""
        x = images
        # è°ƒæ•´åˆ†è¾¨ç‡
        x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)
        # å°†[-1,1]æ˜ å°„åˆ°[0,1]
        x = (x * 0.5) + 0.5
        # å½’ä¸€åŒ–åˆ°ImageNet
        mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1,3,1,1)
        std = torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1,3,1,1)
        x = (x - mean) / std

        if hasattr(self, 'use_transformers') and self.use_transformers:
            outputs = self.backbone(pixel_values=x)
            logits = outputs.logits
        else:
            logits = self.backbone(x)

        batch_size = images.shape[0]
        target_labels = torch.full((batch_size,), 1 if target_detected else 0, dtype=torch.long, device=images.device)
        loss = F.cross_entropy(logits, target_labels)
        return loss

    def compute_adversarial_loss(self, images: torch.Tensor, target_detected: bool = False) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """ç»Ÿä¸€æ¥å£ï¼šè¿”å› (loss, details)"""
        loss = self.compute_detection_loss_diff(images, target_detected=target_detected)
        details = {
            'efficientnet_ce': loss.detach()
        }
        return loss, details

class MultiModalDetector(nn.Module):
    """å¤šæ¨¡æ€æ£€æµ‹å™¨é›†æˆ"""
    
    def __init__(self, 
                 use_yolo: bool = True,   # æ‚¨æœ‰æœ¬åœ°yolov8x.ptï¼Œå¯ä»¥å¯ç”¨
                 use_clip: bool = True,   # å¯ç”¨æœ¬åœ°CLIPæ¨¡å‹
                 use_efficientnet: bool = True,  # ä½¿ç”¨æœ¬åœ°EfficientNet-B4
                 device: str = "cuda"):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€æ£€æµ‹å™¨
        
        Args:
            use_yolo: æ˜¯å¦ä½¿ç”¨YOLOæ£€æµ‹å™¨
            use_clip: æ˜¯å¦ä½¿ç”¨CLIPæ£€æµ‹å™¨
            use_efficientnet: æ˜¯å¦ä½¿ç”¨EfficientNetåˆ†ç±»å™¨
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()
        self.device = device
        self.generator = None # æ·»åŠ ä¸€ä¸ªç”Ÿæˆå™¨çš„å¼•ç”¨
        
        # åˆå§‹åŒ–å„ä¸ªæ£€æµ‹å™¨
        self.detectors = {}
        
        if use_yolo:
            self.detectors['yolo'] = YOLODetector(device=device)
        
        if use_clip:
            self.detectors['clip'] = CLIPDetector(device=device)
            # å†»ç»“åˆ°evalï¼Œç¡®ä¿BN/Dropoutç¨³å®š
            self.detectors['clip'].eval()
        
        if use_efficientnet:
            self.detectors['efficientnet'] = EfficientNetClassifier(device=device)
            self.detectors['efficientnet'].eval()
        
        # æ£€æµ‹å™¨æƒé‡ï¼ˆä»¥YOLOä¸ºä¸»ï¼Œå¯ä»configæ¥å…¥ï¼‰
        self.detector_weights = {
            'yolo': training_config.yolo_branch_weight,
            'clip': training_config.clip_branch_weight,
            'efficientnet': training_config.efficientnet_branch_weight
        }
        
        print(f"å¤šæ¨¡æ€æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æ£€æµ‹å™¨: {list(self.detectors.keys())}")
    
    def forward(self, images: torch.Tensor, masks: Optional[torch.Tensor] = None) -> Dict:
        """
        å¤šæ¨¡æ€æ£€æµ‹å‰å‘æ¨ç†
        
        Args:
            images: è¾“å…¥å›¾åƒå¼ é‡
            masks: å¯é€‰æ©ç ï¼Œä»…ç”¨äºYOLOåˆ†æ”¯åœ¨æ©ç åŒºåŸŸå†…æ£€æµ‹
            
        Returns:
            ç»¼åˆæ£€æµ‹ç»“æœ
        """
        results = {}
        detection_scores = []
        
        # è¿è¡Œå„ä¸ªæ£€æµ‹å™¨
        for detector_name, detector in self.detectors.items():
            # å°†æ©ç ä¼ é€’ç»™YOLOåˆ†æ”¯ï¼Œå…¶å®ƒåˆ†æ”¯å¿½ç•¥æ©ç 
            if detector_name == 'yolo':
                detector_result = detector(images, masks)
            else:
                detector_result = detector(images)
            results[detector_name] = detector_result
            
            # æå–æ£€æµ‹åˆ†æ•°
            if detector_name == 'yolo':
                # YOLO: åŸºäºæ˜¯å¦æœ‰æ£€æµ‹æ¡† + ç½®ä¿¡åº¦çš„è½¯åˆ†æ•°
                soft_scores = []
                for i, detected in enumerate(detector_result['detected']):
                    if detected:
                        # å–è¯¥å›¾åƒæœ€å¤§ç½®ä¿¡åº¦ä½œä¸ºå¼ºåº¦
                        confs = detector_result['scores'][i]
                        max_conf = float(confs.max()) if len(confs) > 0 else 1.0
                        soft_scores.append(max(0.5, max_conf))  # è‡³å°‘0.5ï¼Œè¡¨ç¤ºè¢«æ£€æµ‹
                    else:
                        soft_scores.append(0.0)
                scores = torch.tensor(soft_scores, device=images.device)
            elif detector_name == 'clip':
                # CLIP: åŸºäºç½®ä¿¡åº¦
                scores = torch.sigmoid(detector_result['confidence'])
            elif detector_name == 'efficientnet':
                # EfficientNet: åŸºäºæ¦‚ç‡
                scores = detector_result['probabilities'][:, 1]  # è¿·å½©ç±»åˆ«çš„æ¦‚ç‡
            
            detection_scores.append(scores * self.detector_weights[detector_name])
        
        # èåˆæ£€æµ‹ç»“æœ
        if detection_scores:
            combined_scores = torch.stack(detection_scores, dim=0).sum(dim=0)
            combined_detected = combined_scores > 0.5
        else:
            combined_scores = torch.zeros(images.shape[0], device=images.device)
            combined_detected = torch.zeros(images.shape[0], dtype=torch.bool, device=images.device)
        
        results['combined'] = {
            'scores': combined_scores,
            'detected': combined_detected,
            'confidence': combined_scores
        }
        
        return results
    
    def compute_adversarial_loss(self, images: torch.Tensor, masks: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®—å¯¹æŠ—æŸå¤±ï¼ˆç›®æ ‡æ˜¯ä¸è¢«æ£€æµ‹åˆ°ï¼‰
        
        Args:
            images: è¾“å…¥å›¾åƒå¼ é‡
            
        Returns:
            å¯¹æŠ—æŸå¤±
        """
        # ä»è¾“å…¥å›¾åƒä¸­æå–èƒŒæ™¯ï¼Œç”¨äºCLIPæŸå¤±
        background_images = None
        if masks is not None:
            background_images = images * (1.0 - masks.float())

        # 1) YOLO ç½®ä¿¡åº¦æŸå¤± (æ–¹æ¡ˆA)
        yolo_conf_loss = torch.tensor(0.0, device=images.device)
        yolo_details = {}
        if 'yolo' in self.detectors:
            try:
                # ç›´æ¥è°ƒç”¨YOLOæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚è™½ç„¶å…¶ä¸­æœ‰NMSç­‰ä¸å¯å¾®æ“ä½œï¼Œ
                # ä½†ç½®ä¿¡åº¦åˆ†æ•°é€šå¸¸ä»èƒ½ä¼ é€’ä¸€éƒ¨åˆ†ç¨€ç–çš„æ¢¯åº¦ã€‚
                yolo_results = self.detectors['yolo'].model(images, verbose=False)
                
                all_confidences = []
                for res in yolo_results:
                    if res.boxes is not None and len(res.boxes.conf) > 0:
                        all_confidences.append(res.boxes.conf)
                
                if len(all_confidences) > 0:
                    # å°†æ‰€æœ‰æ£€æµ‹åˆ°çš„ç½®ä¿¡åº¦æ‹¼æ¥èµ·æ¥ï¼Œè®¡ç®—å¹³å‡å€¼ä½œä¸ºæŸå¤±
                    yolo_conf_loss = torch.cat(all_confidences).mean()
                
                yolo_details = {'yolo_conf_mean': yolo_conf_loss.detach()}

            except Exception as e:
                # å¦‚æœYOLOå‰å‘å¤±è´¥ï¼Œæ‰“å°è­¦å‘Šä½†ä¸ä¸­æ–­è®­ç»ƒ
                print(f"âš ï¸ YOLO confidence loss calculation failed: {e}")

        # 2) CLIPå¯å¾®æŸå¤±ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        clip_loss = torch.tensor(0.0, device=images.device)
        clip_details = {}
        if 'clip' in self.detectors and hasattr(self.detectors['clip'], 'compute_adversarial_loss') and background_images is not None:
            clip_loss, clip_details = self.detectors['clip'].compute_adversarial_loss(images, background_images, target_detected=False)
        
        # 3) EfficientNetå¯å¾®æŸå¤±ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        eff_loss = torch.tensor(0.0, device=images.device)
        eff_details = {}
        if 'efficientnet' in self.detectors and hasattr(self.detectors['efficientnet'], 'compute_adversarial_loss'):
            eff_loss, eff_details = self.detectors['efficientnet'].compute_adversarial_loss(images, target_detected=False)

        # æŒ‰configæƒé‡èåˆ
        total_loss = (
            training_config.yolo_adv_weight * yolo_conf_loss +
            training_config.clip_adv_weight * clip_loss +
            training_config.efficientnet_adv_weight * eff_loss
        )
        details = {}
        details.update(yolo_details)
        details.update({f'clip_{k}': v for k, v in clip_details.items()})
        details.update({f'eff_{k}': v for k, v in eff_details.items()})
        return total_loss, details
    
    def evaluate_detection_success(self, images: torch.Tensor, masks: Optional[torch.Tensor] = None) -> Dict:
        """
        è¯„ä¼°æ£€æµ‹æˆåŠŸç‡
        
        Args:
            images: è¾“å…¥å›¾åƒå¼ é‡
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        results = self.forward(images, masks)
        
        evaluation = {
            'total_images': images.shape[0],
            'detected_count': 0,
            'detection_rate': 0.0,
            'per_detector_results': {}
        }
        
        # ç»Ÿè®¡å„æ£€æµ‹å™¨ç»“æœ
        for detector_name in self.detectors.keys():
            detector_result = results[detector_name]
            # å°†boolåˆ—è¡¨/å¼ é‡å®‰å…¨è½¬æ¢
            det = detector_result['detected']
            if isinstance(det, torch.Tensor):
                detected_count = int(det.sum().item())
            else:
                detected_count = int(sum(det))
            detection_rate = detected_count / images.shape[0]
            
            evaluation['per_detector_results'][detector_name] = {
                'detected_count': detected_count,
                'detection_rate': detection_rate
            }
        
        # ç»¼åˆç»“æœ
        comb = results['combined']['detected']
        if isinstance(comb, torch.Tensor):
            combined_detected = int(comb.sum().item())
        else:
            combined_detected = int(sum(comb))
        evaluation['detected_count'] = combined_detected
        evaluation['detection_rate'] = combined_detected / images.shape[0]
        
        return evaluation

if __name__ == "__main__":
    # æµ‹è¯•å¤šæ¨¡æ€æ£€æµ‹å™¨
    print("æµ‹è¯•å¤šæ¨¡æ€AIæ£€æµ‹å™¨...")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    detector = MultiModalDetector(device=device)
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_images = torch.randn(2, 3, 1024, 1024).to(device)
    test_images = test_images * 0.5 + 0.5  # å½’ä¸€åŒ–åˆ°[-1,1]
    test_images = (test_images - 0.5) / 0.5
    
    # æµ‹è¯•æ£€æµ‹
    print("è¿è¡Œæ£€æµ‹...")
    results = detector(test_images)
    print(f"æ£€æµ‹ç»“æœ: {results['combined']['detected']}")
    
    # æµ‹è¯•å¯¹æŠ—æŸå¤±
    print("è®¡ç®—å¯¹æŠ—æŸå¤±...")
    loss = detector.compute_adversarial_loss(test_images)
    print(f"å¯¹æŠ—æŸå¤±: {loss.item()}")
    
    print("AIæ£€æµ‹å™¨æµ‹è¯•å®Œæˆï¼")
