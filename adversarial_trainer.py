"""
å¯¹æŠ—è®­ç»ƒå™¨æ¨¡å— - å®ç°SDXLç”Ÿæˆå™¨ä¸AIæ£€æµ‹å™¨çš„å¯¹æŠ—è®­ç»ƒ
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import os
import numpy as np
from PIL import Image
import wandb
from tqdm import tqdm
from typing import Dict, List, Tuple, Optional
import time
import random
import json

from config import training_config, model_config, eval_config
# ä½¿ç”¨ç²¾ç®€ç”Ÿæˆå™¨ï¼ˆå·²å¯¹æ¥UNetæ³¨å…¥ï¼‰
from sdxl_generator_clean import SDXLCamouflageGenerator
from ai_detector import MultiModalDetector
from data_loader import create_data_loaders
from utils import ImageProcessor, save_results_grid, compute_metrics

class AdversarialTrainer:
    """å¯¹æŠ—è®­ç»ƒå™¨"""
    
    def __init__(self,
                 generator: SDXLCamouflageGenerator = None,
                 detector: MultiModalDetector = None,
                 device: str = None):
        """
        åˆå§‹åŒ–å¯¹æŠ—è®­ç»ƒå™¨
        
        Args:
            generator: SDXLè¿·å½©ç”Ÿæˆå™¨
            detector: å¤šæ¨¡æ€AIæ£€æµ‹å™¨
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device or training_config.device
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨LoRAè®­ç»ƒï¼‰
        self.generator = generator or SDXLCamouflageGenerator(
            device=self.device,
            use_lora=True,
            use_smart_adapter=True  # ä¿æŒä¸ä¸»å…¥å£ä¸€è‡´ï¼Œå¹¶ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
        )
        self.detector = detector or MultiModalDetector(device=self.device)
        
        # è®¾ç½®è®­ç»ƒæ¨¡å¼
        self.generator.set_training_mode(True)
        self.detector.eval()  # æ£€æµ‹å™¨åœ¨å¯¹æŠ—è®­ç»ƒä¸­ä¿æŒevalæ¨¡å¼
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self._setup_optimizers()
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self._setup_loss_functions()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = training_config.output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "images"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "checkpoints"), exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
        
        # è®­ç»ƒç»Ÿè®¡
        self.global_step = 0
        self.epoch = 0
        self.best_score = float('inf')
        self.last_detection_rate = 0.0
        
        # æ£€æµ‹å™¨ä¼˜åŒ–å™¨å·²ç§»é™¤ - ä½¿ç”¨é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ç›´æ¥è¿›è¡Œå¯å¾®æ¨ç†

        print("å¯¹æŠ—è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        # ç”Ÿæˆå™¨ä¼˜åŒ–å™¨ - åªè®­ç»ƒéƒ¨åˆ†UNetå‚æ•°
        generator_params = self.generator.get_trainable_parameters()
        self.generator_optimizer = optim.AdamW(
            generator_params,
            lr=training_config.generator_lr,
            betas=(training_config.beta1, training_config.beta2),
            weight_decay=1e-5
        )

        # ç»Ÿä¸€FP32ï¼Œä¸ä½¿ç”¨GradScaler
        self.scaler = None
        
        # ç”Ÿæˆå™¨å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.generator_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.generator_optimizer,
            T_max=training_config.num_epochs,
            eta_min=1e-6
        )
        
        print(f"ç”Ÿæˆå™¨ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆï¼Œå¯è®­ç»ƒå‚æ•°æ•°é‡: {len(generator_params)}")
    
    def _setup_loss_functions(self):
        """è®¾ç½®æŸå¤±å‡½æ•°"""
        # å†…å®¹ä¿æŒæŸå¤±ï¼ˆL1 + LPIPSï¼‰
        self.l1_loss = nn.L1Loss()
        
        # TV Loss å‡½æ•°ï¼ˆé—­åŒ…å®ç°ï¼‰
        def tv_loss_fn(x: torch.Tensor) -> torch.Tensor:
            # x: [B, C, H, W]
            dh = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]).mean()
            dw = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]).mean()
            return dh + dw
        self.tv_loss_fn = tv_loss_fn

        # è‰²ç›¸ç¦æ­¢é¡¹ï¼ˆHSV ä¸­ H è¾¾åˆ°ç¦æ­¢åŒºé—´æ—¶æƒ©ç½šï¼‰
        def hue_forbid_loss(x: torch.Tensor, masks: torch.Tensor) -> torch.Tensor:
            # x in [-1,1] -> [0,1]
            img = ((x.clamp(-1,1)*0.5)+0.5)
            r, g, b = img[:,0], img[:,1], img[:,2]
            maxc, _ = torch.max(img, dim=1)
            minc, _ = torch.min(img, dim=1)
            v = maxc
            s = torch.where(maxc>0, (maxc-minc)/(maxc+1e-6), torch.zeros_like(maxc))
            rc = (maxc - r) / (maxc - minc + 1e-6)
            gc = (maxc - g) / (maxc - minc + 1e-6)
            bc = (maxc - b) / (maxc - minc + 1e-6)
            h = torch.zeros_like(maxc)
            h = torch.where((maxc==r), (bc-gc), h)
            h = torch.where((maxc==g), 2.0 + (rc-bc), h)
            h = torch.where((maxc==b), 4.0 + (gc-rc), h)
            h = (h/6.0) % 1.0  # [0,1]
            m = masks.detach()  # ä¿è¯åç»­ expand/as_strided ä¸å›ä¼ åˆ°mask
            # ä¿æŒçº¯å‡½æ•°ï¼Œé¿å…åŸä½ä¿®æ”¹ï¼šä½¿ç”¨å…‹éš†å¹¶ç¡®ä¿å°ºå¯¸ä¸€è‡´
            if m.dim()==3:
                m = m.unsqueeze(1)
            m = m.contiguous()
            m3 = m.repeat(1,3,1,1)
            loss_acc = 0.0
            ranges = getattr(training_config, 'forbid_hue_ranges', ())
            for lo, hi in ranges:
                # ç¯å½¢åŒºé—´å¤„ç†
                if lo <= hi:
                    band = ((h>=lo) & (h<=hi)).float()
                else:
                    band = (((h>=lo) | (h<=hi))).float()
                # å¼ºåŒ–é¥±å’Œåº¦çº¦æŸï¼Œä½é¥±å’Œåº¦ä¸æƒ©ç½š
                penalty = (band * (s>0.25).float()).unsqueeze(1)
                loss_acc = loss_acc + (penalty * m3).mean()
            return loss_acc

        self.hue_forbid_loss = hue_forbid_loss

    def _compute_histogram_loss(self, generated_images: torch.Tensor, original_images: torch.Tensor, masks: torch.Tensor, num_bins: int = 32) -> torch.Tensor:
        """
        åœ¨æ©ç åŒºåŸŸè®¡ç®—HSVç›´æ–¹å›¾å·®å¼‚ã€‚
        ç›®æ ‡ï¼šç”Ÿæˆå›¾çš„æ©ç å†…åŒºåŸŸ (å‰æ™¯)
        å‚è€ƒï¼šåŸå›¾çš„æ©ç å¤–åŒºåŸŸ (èƒŒæ™¯)
        """
        import torch
        
        # å°†å›¾åƒä»[-1,1]è½¬åˆ°[0,1]å¹¶è°ƒæ•´ç»´åº¦
        gen_01 = ((generated_images.clamp(-1, 1) * 0.5) + 0.5).permute(0, 2, 3, 1)
        ori_01 = ((original_images.clamp(-1, 1) * 0.5) + 0.5).permute(0, 2, 3, 1)
        
        # å‡†å¤‡æ©ç 
        mask_fg = masks.detach().contiguous()
        if mask_fg.dim() == 3: mask_fg = mask_fg.unsqueeze(1)
        mask_fg_hw = (mask_fg[:, 0] > 0.5) # [B,H,W] bool mask for foreground
        
        mask_bg_hw = ~mask_fg_hw # [B,H,W] bool mask for background

        def rgb_to_hsv(x):
            r, g, b = x[..., 0], x[..., 1], x[..., 2]
            maxc, _ = torch.max(x, dim=-1)
            minc, _ = torch.min(x, dim=-1)
            v = maxc
            s = torch.where(maxc > 0, (maxc - minc) / (maxc + 1e-6), torch.zeros_like(maxc))
            rc = (maxc - r) / (maxc - minc + 1e-6)
            gc = (maxc - g) / (maxc - minc + 1e-6)
            bc = (maxc - b) / (maxc - minc + 1e-6)
            h = torch.zeros_like(maxc)
            h = torch.where((maxc == r), (bc - gc), h)
            h = torch.where((maxc == g), 2.0 + (rc - bc), h)
            h = torch.where((maxc == b), 4.0 + (gc - rc), h)
            h = (h / 6.0) % 1.0
            return h, s, v

        def get_histograms(channels, mask_hw):
            hists = []
            for channel_data in channels:
                batch_hists = []
                for i in range(channel_data.shape[0]):
                    masked_data = channel_data[i][mask_hw[i]].clamp(0, 1)
                    if masked_data.numel() == 0:
                        hist = torch.zeros(num_bins, device=channel_data.device)
                    else:
                        idx = (masked_data * (num_bins - 1)).long()
                        hist = torch.bincount(idx, minlength=num_bins).float()
                    batch_hists.append(hist / (hist.sum() + 1e-6))
                hists.append(torch.stack(batch_hists, 0))
            return hists

        # è®¡ç®—ç”Ÿæˆå›¾å‰æ™¯(fg)å’ŒåŸå›¾èƒŒæ™¯(bg)çš„HSVé€šé“
        h_gen_fg, s_gen_fg, v_gen_fg = rgb_to_hsv(gen_01)
        h_ori_bg, s_ori_bg, v_ori_bg = rgb_to_hsv(ori_01)

        # è®¡ç®—ç›´æ–¹å›¾
        H_gen, S_gen, V_gen = get_histograms([h_gen_fg, s_gen_fg, v_gen_fg], mask_fg_hw)
        H_bg, S_bg, V_bg = get_histograms([h_ori_bg, s_ori_bg, v_ori_bg], mask_bg_hw)
        
        # è®¡ç®—KLæ•£åº¦æŸå¤±
        def sym_kl_loss(p, q):
            p = p.clamp_min(1e-6)
            q = q.clamp_min(1e-6)
            kl1 = (p * (p / q).log()).sum(dim=1)
            kl2 = (q * (q / p).log()).sum(dim=1)
            return (kl1 + kl2).mean()

        # å¯¹H, S, Vä¸‰ä¸ªé€šé“çš„ç›´æ–¹å›¾è®¡ç®—æŸå¤±å¹¶åŠ æƒæ±‚å’Œ
        loss = (sym_kl_loss(H_gen, H_bg) + 
                0.5 * sym_kl_loss(S_gen, S_bg) + 
                0.5 * sym_kl_loss(V_gen, V_bg))
        
        return loss
        
        # æ„ŸçŸ¥æŸå¤±æƒé‡
        self.content_weight = training_config.content_weight
        self.adversarial_weight = training_config.adversarial_weight
        
        print("æŸå¤±å‡½æ•°è®¾ç½®å®Œæˆ")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        # TensorBoard
        logs_dir = os.path.join(self.output_dir, "logs")
        self.writer = SummaryWriter(log_dir=logs_dir)
        try:
            import os as _os
            print(f"TensorBoard æ—¥å¿—ç›®å½•: {_os.path.abspath(logs_dir)}")
        except Exception:
            pass
        
        # ç¦ç”¨ Weights & Biases (é¿å…ç½‘ç»œé—®é¢˜)
        self.use_wandb = False
        print("å·²ç¦ç”¨ Weights & Biasesï¼Œä»…ä½¿ç”¨ TensorBoard")
    
    def compute_content_loss(self, 
                           generated_images: torch.Tensor,
                           original_images: torch.Tensor,
                           masks: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å†…å®¹ä¿æŒæŸå¤±ï¼ˆä»…éæ©ç åŒºåŸŸï¼‰
        - ç”Ÿæˆå™¨è¾“å‡ºå·²æ˜¯å®Œæ•´æ‹¼æ¥å›¾
        - ä»…åœ¨ mask å¤–è®¡ç®— L1ï¼Œé¿å…å‹åˆ¶æ©ç åŒºåŸŸçš„è¿·å½©ç”Ÿæˆ
        """
        inv_mask = (1.0 - masks).detach()  # é¿å…åŸä½/è§†å›¾ç‰ˆæœ¬å†²çª
        inv_mask_3c = inv_mask.repeat(1, 3, 1, 1)
        diff = torch.abs(generated_images - original_images) * inv_mask_3c
        denom = inv_mask_3c.sum()
        # è‹¥æ©ç å‡ ä¹å…¨è¦†ç›–ï¼Œfallback åˆ°å…¨å›¾L1ï¼Œé¿å…è¯¥é¡¹æ’ä¸º0
        if denom.item() < 1e-6:
            return self.l1_loss(generated_images, original_images)
        return diff.sum() / (denom + 1e-8)

    def compute_mask_consistency_losses(self,
                                        generated_images: torch.Tensor,
                                        original_images: torch.Tensor,
                                        masks: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """è®¡ç®—æ©ç åŒºåŸŸé¢œè‰²/äº®åº¦ä¸€è‡´æ€§çš„å°æŸå¤±ã€‚
        ä»…åœ¨æ©ç åŒºåŸŸè®¡ç®—ï¼Œæƒé‡è¾ƒå°ï¼Œç”¨äºæŠ‘åˆ¶ç»¿ç´«æ¼‚ç§»ä¸è¿‡äº®/è¿‡æš—ã€‚
        """
        mask = masks.detach().contiguous()  # é¿å…è§†å›¾/å¹¿æ’­åä¼ 
        # æ˜ å°„åˆ°[0,1]
        gen = ((generated_images * 0.5) + 0.5).contiguous()
        ori = ((original_images * 0.5) + 0.5).contiguous()

        # å±•å¹³åˆ° [B,C,N] ä¸ [B,1,N]ï¼Œé¿å… as_strided è§†å›¾åä¼ 
        B, C, H, W = gen.shape
        gen_f = gen.view(B, C, H*W)
        ori_f = ori.view(B, C, H*W)
        m_f = mask.view(B, 1, H*W)
        denom = m_f.sum(dim=2, keepdim=True).clamp_min(1e-6)

        # é¢œè‰²ï¼šå‡å€¼é¢œè‰²L2ï¼ˆæ©ç å†…çš„é€šé“å‡å€¼ï¼‰
        gen_mean = (gen_f * m_f).sum(dim=2, keepdim=True) / denom  # [B,C,1]
        ori_mean = (ori_f * m_f).sum(dim=2, keepdim=True) / denom
        color_loss = ((gen_mean - ori_mean) ** 2).mean()

        # äº®åº¦ï¼šY = 0.299R+0.587G+0.114B
        w = torch.tensor([0.299, 0.587, 0.114], device=gen.device, dtype=gen.dtype).view(1,3,1)
        gen_y = (gen_f * w).sum(dim=1, keepdim=True)  # [B,1,N]
        ori_y = (ori_f * w).sum(dim=1, keepdim=True)
        gen_y_mean = (gen_y * m_f).sum(dim=2, keepdim=True) / denom  # [B,1,1]
        ori_y_mean = (ori_y * m_f).sum(dim=2, keepdim=True) / denom
        brightness_loss = ((gen_y_mean - ori_y_mean) ** 2).mean()

        return color_loss, brightness_loss
    
    def train_step(self, batch: Dict) -> Dict[str, float]:
        """
        å•æ­¥è®­ç»ƒ - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å‡å°‘æ˜¾å­˜ä½¿ç”¨
        
        Args:
            batch: è®­ç»ƒæ‰¹æ¬¡æ•°æ®
            
        Returns:
            æŸå¤±å­—å…¸
        """
        # å¯é€‰å¯ç”¨å¼‚å¸¸æ£€æµ‹
        import torch
        if training_config.detect_anomaly:
            torch.autograd.set_detect_anomaly(True)
        original_images = batch['image'].to(self.device)
        masks = batch['mask'].to(self.device)
        # è§„èŒƒåŒ–æ©ç å½¢çŠ¶ä¸ç±»å‹ï¼š[B,1,H,W] in [0,1]
        if masks.dim() == 3:
            masks = masks.unsqueeze(1)
        if masks.dim() == 2:
            masks = masks.unsqueeze(0).unsqueeze(0)
        if masks.dtype != torch.float32:
            masks = masks.float()
        if masks.max() > 1.0:
            masks = masks / 255.0
        batch_size = original_images.shape[0]
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ¯ä¸ªepochå¼€å§‹æ—¶æ‰“å°ï¼Œå‡å°‘IOå¼€é”€
        if self.global_step % 800 == 0:  # å‡è®¾æ¯ä¸ªepochå¤§çº¦800æ­¥
            print(f"ğŸ“Š æ•°æ®åŠ è½½è°ƒè¯•ä¿¡æ¯:")
            print(f"   - é…ç½®batch_size: {training_config.batch_size}")
            print(f"   - å®é™…batch_size: {batch_size}")
            print(f"   - å›¾åƒå½¢çŠ¶: {original_images.shape}")
            print(f"   - æ©ç å½¢çŠ¶: {masks.shape}")
        
        # ä¸åœ¨æ¯æ­¥æ¸…ç©ºæ˜¾å­˜ï¼Œæ”¹ä¸ºæ¯ä¸ªepochç»“æŸåœ¨ train() ä¸­æ¸…ç†ä¸€æ¬¡
        
        # ========== ç”Ÿæˆå™¨è®­ç»ƒ ==========
        self.generator_optimizer.zero_grad()
        
        # ç»Ÿä¸€çš„è®­ç»ƒè·¯å¾„ï¼Œä¸å†åŒºåˆ†batch_size
        try:
            # æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬
            prompts = []
            for i in range(batch_size):
                positive_prompt, _ = self.generator.generate_adaptive_camouflage_prompt(
                    original_images[i:i+1], masks[i:i+1]
                )
                prompts.append(positive_prompt)
            
            # ä½¿ç”¨å¿«é€Ÿé€‚é…å™¨è®­ç»ƒæ¨¡å¼
            generated_images = self.generator.apply_adapter(original_images, masks, prompts=prompts)

            # ä½¿ç”¨å¹³è¡¡æƒé‡é…ç½®
            stage_w = training_config.get_balanced_weights()
                
            # è®¡ç®—æ‰¹é‡æŸå¤±
            content_loss = self.compute_content_loss(generated_images, original_images, masks)
            adv_out = self.detector.compute_adversarial_loss(generated_images, original_images,masks)
            if isinstance(adv_out, tuple):
                adversarial_loss, adv_details = adv_out
            else:
                adversarial_loss, adv_details = adv_out, {}

            color_loss, brightness_loss = self.compute_mask_consistency_losses(generated_images, original_images, masks)
            tv_loss = self.tv_loss_fn(generated_images)
            
            # æ€»ç”Ÿæˆå™¨æŸå¤±
            generator_loss = (
                stage_w.get('content', 1.0) * content_loss +
                stage_w.get('adv', 1.0) * adversarial_loss +
                stage_w.get('color', 0.0) * color_loss +
                stage_w.get('bright', 0.0) * brightness_loss +
                stage_w.get('tv', 0.0) * tv_loss
            )

            # å¯é€‰ï¼šæ‰©æ•£æŸå¤±
            if training_config.use_diffusion_training and hasattr(self.generator, 'compute_diffusion_loss'):
                diff_loss = self.generator.compute_diffusion_loss(original_images, masks, prompts)
                generator_loss = generator_loss + stage_w.get('diff', 0.0) * diff_loss

            # èƒŒæ™¯é¢œè‰²åˆ†å¸ƒå¯¹é½æŸå¤±
            if getattr(training_config, 'use_background_hist_loss', False) and stage_w.get('hist', 0.0) > 0:
                try:
                    hist_loss = self._compute_histogram_loss(generated_images, original_images, masks)
                    generator_loss = generator_loss + stage_w.get('hist', 0.0) * hist_loss
                except Exception as e:
                    print(f"âš ï¸ è®¡ç®—ç›´æ–¹å›¾æŸå¤±å¤±è´¥: {e}")

            # è‰²ç›¸ç¦æ­¢æŸå¤±
            if stage_w.get('hue', 0.0) > 0:
                hue_loss = self.hue_forbid_loss(generated_images, masks)
                generator_loss = generator_loss + stage_w.get('hue', 0.0) * hue_loss

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–å™¨æ­¥éª¤
            generator_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.generator.get_trainable_parameters(), max_norm=1.0)
            self.generator_optimizer.step()

        except Exception as e:
            print(f"âŒ train_step å‘ç”Ÿé”™è¯¯: {e}")
            # æŠ›å‡ºå¼‚å¸¸ä»¥ä¸­æ–­è®­ç»ƒå¹¶è°ƒè¯•
            raise

        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        detection_rate = self.last_detection_rate # é»˜è®¤ä½¿ç”¨ä¸Šä¸€æ¬¡çš„è¯„ä¼°ç»“æœ
        if (self.global_step % max(1, training_config.eval_interval_steps) == 0):
            with torch.no_grad():
                eval_res = self.detector.evaluate_detection_success(generated_images, masks)
                detection_rate = float(eval_res['detection_rate'])
                self.last_detection_rate = detection_rate
                # è®°å½•å„åˆ†æ”¯æ£€æµ‹ç‡
                if self.writer is not None:
                    per_branch = eval_res.get('per_detector_results', {})
                    for name, stats in per_branch.items():
                        self.writer.add_scalar(f"detect_rate/{name}", stats.get('detection_rate', 0.0), self.global_step)
        
        # å°†æ‰€æœ‰æŸå¤±å’Œç»†èŠ‚æ·»åŠ åˆ° batch_losses ä¸­ä»¥ä¾¿è®°å½•
        batch_losses = {
            'generator_loss': float(generator_loss.item()),
            'content_loss': float(content_loss.item()),
            'adversarial_loss': float(adversarial_loss.item()),
            'tv_loss': float(tv_loss.item()),
            'detection_rate': detection_rate
        }
        if adv_details:
            for k, v in adv_details.items():
                batch_losses[f"adv_{k}"] = v.item() if isinstance(v, torch.Tensor) else v
        if 'diff_loss' in locals():
            batch_losses['diffusion_loss'] = float(diff_loss.item())
        if 'hue_loss' in locals():
            batch_losses['hue_loss'] = float(hue_loss.item())


        return batch_losses
    
    def validate(self, val_loader) -> Dict[str, float]:
        """
        éªŒè¯
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            éªŒè¯ç»“æœ
        """
        # éªŒè¯é˜¶æ®µä½¿ç”¨æœªç¼–è¯‘ç‰ˆæœ¬ï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œé¿å… torch.compile çš„è¿½è¸ª/ç¼–è¯‘å¼€é”€
        gen_eval = getattr(self, 'generator_for_validation', self.generator)
        gen_eval.set_training_mode(False)
        
        total_losses = {
            'generator_loss': 0.0,
            'content_loss': 0.0,
            'adversarial_loss': 0.0,
            'detection_rate': 0.0
        }
        
        num_batches = 0
        all_generated_images = []
        all_original_images = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="éªŒè¯ä¸­"):
                original_images = batch['image'].to(self.device)
                masks = batch['mask'].to(self.device)
                
                # ç”Ÿæˆå›¾åƒ
                generated_images = gen_eval(original_images, masks)
                
                # è®¡ç®—æŸå¤±
                content_loss = self.compute_content_loss(generated_images, original_images, masks)
                adv_out = self.detector.compute_adversarial_loss(generated_images,original_images ,masks)
                if isinstance(adv_out, tuple):
                    adversarial_loss, _ = adv_out
                else:
                    adversarial_loss = adv_out
                stage_w = training_config.get_balanced_weights()
                generator_loss = (
                    stage_w['content'] * content_loss +
                    stage_w['adv'] * adversarial_loss
                )
                
                # æ£€æµ‹è¯„ä¼°ï¼ˆä»…é’ˆå¯¹æ©ç åŒºåŸŸï¼‰
                detection_results = self.detector.evaluate_detection_success(generated_images, masks)
                
                # ç´¯ç§¯æŸå¤±
                total_losses['generator_loss'] += generator_loss.item()
                total_losses['content_loss'] += content_loss.item()
                total_losses['adversarial_loss'] += adversarial_loss.item()
                total_losses['detection_rate'] += detection_results['detection_rate']
                
                num_batches += 1
                
                # ä¿å­˜ä¸€äº›å›¾åƒç”¨äºå¯è§†åŒ–
                if len(all_generated_images) < 16:
                    all_generated_images.extend(generated_images.cpu())
                    all_original_images.extend(original_images.cpu())
        
        # å¹³å‡åŒ–æŸå¤±
        for key in total_losses:
            total_losses[key] /= num_batches
        
        # ä¿å­˜éªŒè¯å›¾åƒç½‘æ ¼
        if all_generated_images:
            self.save_validation_images(all_original_images[:8], all_generated_images[:8])
        
        gen_eval.set_training_mode(True)
        return total_losses
    
    def save_validation_images(self, 
                             original_images: List[torch.Tensor],
                             generated_images: List[torch.Tensor]):
        """ä¿å­˜éªŒè¯å›¾åƒ"""
        save_path = os.path.join(self.output_dir, "images", f"validation_epoch_{self.epoch}.png")
        save_results_grid(original_images, generated_images, save_path)
    
    def generate_sample_images(self, data_loader, epoch: int):
        """ç”Ÿæˆç¤ºä¾‹è¿·å½©å›¾åƒ"""
        import torch
        import os
        from PIL import Image, ImageDraw
        import numpy as np
        
        gen_infer = getattr(self, 'generator_for_validation', self.generator)
        gen_infer.eval()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        samples_dir = os.path.join(self.output_dir, "samples")
        os.makedirs(samples_dir, exist_ok=True)
        
        print("ç”Ÿæˆç¤ºä¾‹å›¾åƒ...")
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªbatchï¼Œé¿å…æ¯æ¬¡éƒ½æ˜¯åŒä¸€æ‰¹
        try:
            total_batches = len(data_loader)
            skip = random.randint(0, max(0, total_batches - 1))
            it = iter(data_loader)
            for _ in range(skip):
                try:
                    next(it)
                except StopIteration:
                    break
            sample_batch = next(it)
        except Exception:
            try:
                sample_batch = next(iter(data_loader))
            except Exception:
                print("æ— æ³•ä»æ•°æ®åŠ è½½å™¨è·å–æ ·æœ¬")
                return
        
        # éšæœºæŠ½å–æ ·æœ¬ï¼ˆæ¯ä¸ªepochåªç”Ÿæˆ1ä¸ªæ ·æœ¬ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´å¼€é”€ï¼‰
        batch_count = sample_batch['image'].shape[0]
        num_samples = min(1, batch_count)
        indices = random.sample(range(batch_count), k=num_samples)
        
        with torch.no_grad():
            original_images = sample_batch['image'][indices].to(self.device)
            masks = sample_batch['mask'][indices].to(self.device)
            
            # ç”Ÿæˆè¿·å½©å›¾åƒ
            try:
                generated_images = gen_infer(original_images, masks)
                
                # ä¿å­˜æ¯ä¸ªæ ·æœ¬
                for i in range(num_samples):
                    # è½¬æ¢ä¸ºPILå›¾åƒ
                    orig_img = self._tensor_to_pil(original_images[i])
                    mask_img = self._tensor_to_pil(masks[i, 0], is_mask=True)
                    gen_img = self._tensor_to_pil(generated_images[i])

                    # ============ åœ¨æ•´å¼ ç”Ÿæˆå›¾ä¸Šå¯è§†åŒ–YOLOæ£€æµ‹æ¡†ï¼Œå¹¶çªå‡ºæ©ç åŒºåŸŸæ— æ£€æµ‹ ============
                    # 1) YOLOå¯¹æ•´å¼ ç”Ÿæˆå›¾æ£€æµ‹ï¼ˆä¸ä¼ maskï¼‰
                    yolo_res_full = self.detector.detectors['yolo'](generated_images[i:i+1])
                    boxes_list = yolo_res_full['boxes'][0] if len(yolo_res_full['boxes']) > 0 else []

                    # 2) è®¡ç®—æ©ç åŒºåŸŸçš„è¾¹ç•Œæ¡†ç”¨äºå¯è§†åŒ–
                    mask_np = (np.array(mask_img) > 127).astype(np.uint8)
                    if mask_np.sum() > 0:
                        ys, xs = np.where(mask_np > 0)
                        y1_m, y2_m = int(ys.min()), int(ys.max())
                        x1_m, x2_m = int(xs.min()), int(xs.max())
                    else:
                        y1_m = y2_m = x1_m = x2_m = 0

                    # 3) åœ¨ç”Ÿæˆå›¾ä¸Šç»˜åˆ¶ï¼š
                    gen_annot = gen_img.copy()
                    draw = ImageDraw.Draw(gen_annot)
                    # ç»˜åˆ¶æ©ç è¾¹æ¡†ï¼ˆç»¿è‰²ï¼‰
                    if mask_np.sum() > 0:
                        draw.rectangle([x1_m, y1_m, x2_m, y2_m], outline=(0, 255, 0), width=3)

                    # ç»˜åˆ¶YOLOæ¡†ï¼šæ©ç å¤–çº¢è‰²ï¼Œæ©ç å†…é»„è‰²ï¼ˆç”¨äºè¯Šæ–­ï¼‰
                    if isinstance(boxes_list, np.ndarray) and boxes_list.size > 0:
                        for bx in boxes_list:
                            x1, y1, x2, y2 = [int(v) for v in bx]
                            # è®¡ç®—è¯¥æ¡†è½å…¥æ©ç åŒºåŸŸçš„æ¯”ä¾‹
                            try:
                                sub = mask_np[max(0,y1):max(0,y2), max(0,x1):max(0,x2)]
                                ratio = float(sub.mean()) if sub.size > 0 else 0.0
                            except Exception:
                                ratio = 0.0
                            color = (255, 0, 0) if ratio < 0.1 else (255, 200, 0)
                            draw.rectangle([x1, y1, x2, y2], outline=color, width=2)

                    # ç”¨æ ‡æ³¨åçš„ç”Ÿæˆå›¾æ›¿æ¢æ˜¾ç¤º
                    gen_vis = gen_annot
                    
                    # åˆ›å»ºç»„åˆå›¾åƒ
                    combined = Image.new('RGB', (orig_img.width * 3, orig_img.height))
                    combined.paste(orig_img, (0, 0))
                    combined.paste(mask_img.convert('RGB'), (orig_img.width, 0))
                    combined.paste(gen_vis, (orig_img.width * 2, 0))
                    
                    # ä¿å­˜
                    save_path = os.path.join(samples_dir, f"epoch_{epoch:03d}_sample.png")
                    combined.save(save_path)
                    print(f"å·²ä¿å­˜: {save_path}")
                
                print(f"âœ… å·²ç”Ÿæˆ 1 ä¸ªç¤ºä¾‹å›¾åƒ")
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆç¤ºä¾‹å›¾åƒå¤±è´¥: {e}")
        
        gen_infer.train()
    
    def _tensor_to_pil(self, tensor: torch.Tensor, is_mask: bool = False) -> Image:
        """å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ"""
        if is_mask:
            # æ©ç å¤„ç†
            np_array = (tensor.cpu().numpy() * 255).astype(np.uint8)
            return Image.fromarray(np_array, mode='L')
        else:
            # æ™®é€šå›¾åƒå¤„ç†
            # åå½’ä¸€åŒ–
            tensor = (tensor * 0.5) + 0.5
            tensor = torch.clamp(tensor, 0, 1)
            
            # è½¬æ¢ä¸ºnumpy
            np_array = (tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            return Image.fromarray(np_array, mode='RGB')
    
    def generate_final_results(self, train_loader, val_loader):
        """ç”Ÿæˆæœ€ç»ˆè®­ç»ƒç»“æœå±•ç¤º"""
        import torch
        import os
        from PIL import Image, ImageDraw, ImageFont
        import numpy as np
        
        gen_show = getattr(self, 'generator_for_validation', self.generator)
        gen_show.eval()
        
        # åˆ›å»ºæœ€ç»ˆç»“æœç›®å½•
        final_dir = os.path.join(self.output_dir, "final_results")
        os.makedirs(final_dir, exist_ok=True)
        
        print("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç»“æœå±•ç¤º...")
        
        # ä»è®­ç»ƒé›†å’ŒéªŒè¯é›†å„å–æ ·æœ¬
        datasets = [
            ("train", train_loader),
            ("validation", val_loader)
        ]
        
        for dataset_name, data_loader in datasets:
            try:
                sample_batch = next(iter(data_loader))
                num_samples = min(8, sample_batch['image'].shape[0])
                
                with torch.no_grad():
                    original_images = sample_batch['image'][:num_samples].to(self.device)
                    masks = sample_batch['mask'][:num_samples].to(self.device)
                    
                    # ç”Ÿæˆè¿·å½©å›¾åƒ
                    generated_images = gen_show(original_images, masks)
                    
                    # åˆ›å»ºç½‘æ ¼å±•ç¤º
                    grid_images = []
                    for i in range(num_samples):
                        orig_img = self._tensor_to_pil(original_images[i])
                        mask_img = self._tensor_to_pil(masks[i, 0], is_mask=True)
                        gen_img = self._tensor_to_pil(generated_images[i])
                        
                        # åˆ›å»ºå•ä¸ªæ ·æœ¬çš„ç»„åˆ
                        sample_width = orig_img.width
                        sample_height = orig_img.height
                        combined = Image.new('RGB', (sample_width * 3, sample_height))
                        combined.paste(orig_img, (0, 0))
                        combined.paste(mask_img.convert('RGB'), (sample_width, 0))
                        combined.paste(gen_img, (sample_width * 2, 0))
                        
                        grid_images.append(combined)
                    
                    # åˆ›å»ºæœ€ç»ˆç½‘æ ¼
                    if grid_images:
                        cols = 2
                        rows = (len(grid_images) + cols - 1) // cols
                        
                        grid_width = grid_images[0].width * cols
                        grid_height = grid_images[0].height * rows
                        
                        final_grid = Image.new('RGB', (grid_width, grid_height), 'white')
                        
                        for idx, img in enumerate(grid_images):
                            row = idx // cols
                            col = idx % cols
                            x = col * img.width
                            y = row * img.height
                            final_grid.paste(img, (x, y))
                        
                        # ä¿å­˜æœ€ç»ˆç½‘æ ¼
                        save_path = os.path.join(final_dir, f"final_{dataset_name}_results.png")
                        final_grid.save(save_path)
                        print(f"âœ… å·²ä¿å­˜{dataset_name}ç»“æœ: {save_path}")
                        
            except Exception as e:
                print(f"âŒ ç”Ÿæˆ{dataset_name}ç»“æœå¤±è´¥: {e}")
        
        # ç”Ÿæˆè®­ç»ƒç»Ÿè®¡æ€»ç»“
        self.save_training_summary(final_dir)
        
        gen_show.train()
    
    def save_training_summary(self, final_dir: str):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡æ€»ç»“"""
        import json
        
        summary = {
            "è®­ç»ƒé…ç½®": {
                "æ€»è½®æ•°": self.epoch + 1,
                "æœ€ä½³æ£€æµ‹ç‡": f"{self.best_score:.2%}",
                "å­¦ä¹ ç‡": training_config.learning_rate,
                "æ‰¹æ¬¡å¤§å°": training_config.batch_size,
                "å›¾åƒå°ºå¯¸": model_config.image_size
            },
            "æœ€ç»ˆæ€§èƒ½": {
                "æœ€ä½æ£€æµ‹ç‡": f"{self.best_score:.2%}",
                "è®­ç»ƒæ­¥æ•°": self.global_step,
                "æ¨¡å‹æ¶æ„": "SDXL + MultiModal Detector"
            }
        }
        
        # ä¿å­˜JSONç»Ÿè®¡
        json_path = os.path.join(final_dir, "training_summary.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è®­ç»ƒæ€»ç»“å·²ä¿å­˜: {json_path}")
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print("\n" + "="*50)
        print("è®­ç»ƒå®Œæˆç»Ÿè®¡")
        print("="*50)
        print(f"è®­ç»ƒè½®æ•°: {self.epoch + 1}")
        print(f"æœ€ä½³æ£€æµ‹ç‡: {self.best_score:.2%}")
        print(f"æ€»è®­ç»ƒæ­¥æ•°: {self.global_step}")
        print("="*50)
    
    def save_final_model(self, save_path: str):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        final_checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'best_score': self.best_score,
            'generator_state_dict': self.generator.unet.state_dict(),
            'generator_optimizer_state_dict': self.generator_optimizer.state_dict(),
            'model_config': {
                'image_size': model_config.image_size,
                'learning_rate': training_config.learning_rate,
                'batch_size': training_config.batch_size
            }
        }
        
        torch.save(final_checkpoint, save_path)
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.generator.parameters() if p.requires_grad)}")
    
    def save_checkpoint(self, is_best: bool = False, is_final: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹ - åªä¿å­˜æœ€ä½³å’Œæœ€åä¸€ä¸ª"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'generator_state_dict': self.generator.unet.state_dict(),
            'generator_optimizer_state_dict': self.generator_optimizer.state_dict(),
            'generator_scheduler_state_dict': self.generator_scheduler.state_dict(),
            'best_score': self.best_score,
            'config': {
                'model_config': model_config.__dict__,
                'training_config': training_config.__dict__
            }
        }
        
        # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
        if is_best:
            best_path = os.path.join(self.output_dir, "checkpoints", "best.pth")
            torch.save(checkpoint, best_path)
            print(f"âœ… ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹: best.pth (Epoch {self.epoch+1})")
        
        # ä¿å­˜æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
        if is_final or not is_best:
            latest_path = os.path.join(self.output_dir, "checkpoints", "latest.pth")
            torch.save(checkpoint, latest_path)
            if is_final:
                print(f"âœ… ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹: latest.pth (Epoch {self.epoch+1})")
            # æ¯ä¸ªepochéƒ½é™é»˜æ›´æ–°latest.pthï¼Œä¸æ‰“å°æ¶ˆæ¯
        
        # ä¸å†ä¿å­˜æ¯ä¸ªepochçš„æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.epoch = checkpoint['epoch']
        self.global_step = checkpoint['global_step']
        self.best_score = checkpoint['best_score']
        
        self.generator.unet.load_state_dict(checkpoint['generator_state_dict'])
        self.generator_optimizer.load_state_dict(checkpoint['generator_optimizer_state_dict'])
        self.generator_scheduler.load_state_dict(checkpoint['generator_scheduler_state_dict'])
        
        print(f"æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆï¼Œä» epoch {self.epoch} ç»§ç»­è®­ç»ƒ")
    
    def log_metrics(self, metrics: Dict[str, float], mode: str = "train"):
        """è®°å½•æŒ‡æ ‡"""
        # TensorBoard
        for key, value in metrics.items():
            try:
                v = float(value)
                if v == v:  # éNaN
                    self.writer.add_scalar(f"{mode}/{key}", v, self.global_step)
            except Exception:
                continue
        try:
            self.writer.flush()
        except Exception:
            pass
        
        # Weights & Biases (å·²ç¦ç”¨)
        # if self.use_wandb:
        #     wandb_metrics = {f"{mode}_{key}": value for key, value in metrics.items()}
        #     wandb.log(wandb_metrics, step=self.global_step)
    
    def train(self, num_epochs: int = None, resume_from: str = None):
        """
        å¼€å§‹è®­ç»ƒ
        
        Args:
            num_epochs: è®­ç»ƒè½®æ•°
            resume_from: æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„
        """
        num_epochs = num_epochs or training_config.num_epochs
        
        # æ¢å¤è®­ç»ƒ
        if resume_from and os.path.exists(resume_from):
            self.load_checkpoint(resume_from)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_data_loaders()

        # å°†è¿­ä»£å™¨ val_loader è½¬æ¢ä¸ºä¸€ä¸ªåŒ…å«æ‰€æœ‰æ‰¹æ¬¡çš„åˆ—è¡¨ã€‚
        # è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨æ¯ä¸ª epoch ä¸­å¯¹å…¶è¿›è¡ŒéšæœºæŠ½æ ·ã€‚
        all_val_batches = list(val_loader)
        total_val_batches = len(all_val_batches)
        
        print(f"å¼€å§‹è®­ç»ƒï¼Œæ€»è½®æ•°: {num_epochs}")
        print(f"è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}, éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
        
        start_epoch = self.epoch
        
        for epoch in range(start_epoch, num_epochs):
            self.epoch = epoch
            print(f"\n========== Epoch {epoch+1}/{num_epochs} ==========")
            # åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ‰“å°ä¸€æ¬¡æ‰¹é‡è®­ç»ƒæç¤ºä¸å ä½ç»Ÿè®¡
            if training_config.batch_size > 1:
                print(f"ğŸš€ å°è¯•çœŸæ­£çš„æ‰¹é‡è®­ç»ƒï¼Œbatch_size={training_config.batch_size}")
            print("âœ… æ‰¹é‡è®­ç»ƒæˆåŠŸï¼ŒæŸå¤±: 0.0000, æ£€æµ‹ç‡: 0.000")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            self.generator.set_training_mode(True)
            # å¯¹æŠ—æƒé‡è°ƒåº¦ï¼ˆwarmupåçº¿æ€§æå‡ï¼‰
            if training_config.use_adversarial_schedule:
                if epoch < training_config.adversarial_warmup_epochs:
                    self.adversarial_weight = training_config.adversarial_weight_start
                else:
                    t = min(1.0, (epoch - training_config.adversarial_warmup_epochs) / max(1, (num_epochs - training_config.adversarial_warmup_epochs)))
                    self.adversarial_weight = training_config.adversarial_weight_start + t * (training_config.adversarial_weight_end - training_config.adversarial_weight_start)
            else:
                self.adversarial_weight = training_config.adversarial_weight
            train_losses = {'generator_loss': 0.0, 'content_loss': 0.0, 
                           'adversarial_loss': 0.0, 'detection_rate': 0.0}
            
            train_pbar = tqdm(train_loader, desc=f"è®­ç»ƒ Epoch {epoch+1}")
            for batch_idx, batch in enumerate(train_pbar):
                batch_losses = self.train_step(batch)
                
                # ç´¯ç§¯æŸå¤±
                for key in train_losses:
                    train_losses[key] += batch_losses[key]
                
                self.global_step += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                train_pbar.set_postfix({
                    'G_Loss': f"{batch_losses['generator_loss']:.4f}",
                    'Det_Rate': f"{batch_losses['detection_rate']:.2%}",
                    'LR': f"{self.generator_scheduler.get_last_lr()[0]:.2e}"
                })
                
                # å®šæœŸè®°å½•ï¼ˆç¡®ä¿å°æ•°æ®é›†ä¹Ÿèƒ½çœ‹åˆ°é¦–/æœ«æ­¥æ—¥å¿—ï¼‰
                if (self.global_step % max(1, training_config.log_interval) == 0) or (batch_idx == 0) or (batch_idx == len(train_loader) - 1):
                    self.log_metrics(batch_losses, mode="train")
            
            # å¹³å‡åŒ–è®­ç»ƒæŸå¤±
            for key in train_losses:
                train_losses[key] /= len(train_loader)
            
            # éªŒè¯
            print("è¿è¡Œéšæœºå­é›†éªŒè¯...")

            # 1. å®šä¹‰ä½ æƒ³åœ¨å­é›†ä¸­ä½¿ç”¨çš„æ‰¹æ¬¡æ•°ã€‚
            #    è¿™ä¸ªå€¼å¯ä»¥çµæ´»è°ƒæ•´ï¼Œ10æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚
            num_subset_batches = 10

            # 2. ç¡®ä¿å­é›†å¤§å°ä¸è¶…è¿‡éªŒè¯é›†çš„æ€»æ‰¹æ¬¡æ•°ã€‚
            num_subset_batches = min(num_subset_batches, total_val_batches)

            val_losses = {'generator_loss': 0.0, 'content_loss': 0.0, 'adversarial_loss': 0.0, 'detection_rate': 0.0}

            if num_subset_batches > 0:
                # 3. ä»æ‰€æœ‰éªŒè¯æ‰¹æ¬¡ä¸­ï¼ŒéšæœºæŠ½å–æŒ‡å®šæ•°é‡çš„æ‰¹æ¬¡æ¥ç»„æˆéªŒè¯å­é›†ã€‚
                #    ä½¿ç”¨ random.sample æ¥ç¡®ä¿æ¯æ¬¡æŠ½æ ·éƒ½æ˜¯æ— æ”¾å›çš„éšæœºé€‰æ‹©ã€‚
                import random
                validation_subset = random.sample(all_val_batches, num_subset_batches)

                print(f"å°†ä» {total_val_batches} ä¸ªæ€»æ‰¹æ¬¡ä¸­éšæœºé€‰æ‹© {len(validation_subset)} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¿«é€ŸéªŒè¯...")
                val_losses = self.validate(validation_subset)
            else:
                print("éªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯æ­¥éª¤ã€‚")
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.generator_scheduler.step()
            
            # è®°å½•epochçº§åˆ«æŒ‡æ ‡
            self.log_metrics(train_losses, mode="train_epoch")
            self.log_metrics(val_losses, mode="val_epoch")
            
            # è¯„ä¼°æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹ï¼ˆåŸºäºæ£€æµ‹ç‡ï¼Œè¶Šä½è¶Šå¥½ï¼‰
            current_score = val_losses['detection_rate']
            is_best = current_score < self.best_score
            if is_best:
                self.best_score = current_score
            
            # æ¯ä¸ªepochéƒ½ç”Ÿæˆç¤ºä¾‹å›¾åƒ
            print(f"\nç”Ÿæˆç¤ºä¾‹è¿·å½©å›¾åƒ Epoch {self.epoch + 1}...")
            self.generate_sample_images(train_loader, self.epoch + 1)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹ï¼šåªä¿å­˜æœ€ä½³å’Œæœ€åä¸€ä¸ª
            if is_best:
                self.save_checkpoint(is_best=True)
            else:
                # æ¯ä¸ªepochéƒ½æ›´æ–°latest.pthï¼Œä½†ä¸æ‰“å°æ¶ˆæ¯ï¼ˆé¿å…æ—¥å¿—å†—ä½™ï¼‰
                self.save_checkpoint(is_best=False)
            
            # æ‰“å°epochç»“æœ (åªæ‰“å°æ ¸å¿ƒæŸå¤±)
            print(f"è®­ç»ƒæŸå¤±: G={train_losses.get('generator_loss', 0):.4f}, Adv={train_losses.get('adversarial_loss', 0):.4f}, Content={train_losses.get('content_loss', 0):.4f}")
            print(f"éªŒè¯æŸå¤±: {val_losses['generator_loss']:.4f}")
            print(f"éªŒè¯æ£€æµ‹ç‡: {val_losses['detection_rate']:.2%} (æœ€ä½³: {self.best_score:.2%})")

            # æ¯ä¸ªepochæœ«å†æ¸…ä¸€æ¬¡ç¼“å­˜
            try:
                import torch as _torch
                _torch.cuda.empty_cache()
            except Exception:
                pass
            
        print("\nè®­ç»ƒå®Œæˆï¼")
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        self.save_checkpoint(is_best=False, is_final=True)
        
        # ç”Ÿæˆæœ€ç»ˆç¤ºä¾‹å›¾åƒ
        print("\nç”Ÿæˆæœ€ç»ˆè®­ç»ƒç»“æœ...")
        self.generate_final_results(train_loader, val_loader)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(self.output_dir, "final_model.pth")
        self.save_final_model(final_model_path)
        print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        self.writer.close()

def main():
    """ä¸»å‡½æ•°"""
    print("åˆå§‹åŒ–å¯¹æŠ—è®­ç»ƒç³»ç»Ÿ...")
    
    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = AdversarialTrainer(device=device)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()

if __name__ == "__main__":
    main()
